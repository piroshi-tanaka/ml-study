{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EV長時間放置クラスタ予測ノートブック\n",
        "要件.md（§0〜§8）に沿ってデータ前処理・学習・評価・成果物出力をhashvin単位で実行します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. セットアップ\n",
        "ここではPythonパッケージと社内モジュールを読み込み、要件§0〜§2で求められる『hashvin単位の独立処理』を行う準備をします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b3a1cfb3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# §0〜§2: パス設定とユーティリティ読込（hashvinごとの独立処理を守る前提）\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# ノートブックは multiclass_train ディレクトリで実行する想定（他hashvinの情報を混ぜない）\n",
        "PROJECT_DIR = Path.cwd()\n",
        "if str(PROJECT_DIR.parent) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_DIR.parent))\n",
        "\n",
        "from multiclass_train.pipeline import FeatureToggleConfig, PipelineConfig, HashvinProcessor\n",
        "from multiclass_train.trainer import train_and_evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc9b0b73",
      "metadata": {},
      "source": [
        "## 1. 設定\n",
        "要件§3〜§7で利用するハイパーパラメータ・閾値を定義します。ステーション種別は将来的に有効化できるようフラグで制御します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "22fd861d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# §3〜§7: パイプライン設定（HEAD抽出K、特徴量トグルなど）\n",
        "feature_toggles = FeatureToggleConfig(\n",
        "    use_distance=True,          # MVP: 距離\n",
        "    use_frequency=True,         # MVP: 頻度\n",
        "    use_recency=True,           # MVP: Recency\n",
        "    use_time_compat=True,       # MVP: time_compat\n",
        "    use_behavior_flags=True,    # ＋α: 行動帯フラグ（必要に応じてOFFに切替）\n",
        "    use_station_type=False,     # ＋α: ステーション種別（外部情報追加時にON推奨）\n",
        ")\n",
        "\n",
        "pipeline_config = PipelineConfig(\n",
        "    head_k=10,  # §3: HEADクラスタの上限 K≤10\n",
        "    feature_toggles=feature_toggles,\n",
        ")\n",
        "\n",
        "autogluon_presets = 'medium_quality'  # §6: AutoGluon Tabular のプリセット\n",
        "autogluon_time_limit = 300  # 学習時間の上限（秒）\n",
        "eval_thresholds = (0.5, 0.7, 0.9)  # §7: Top-1@τ の閾値候補"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. データ読込\n",
        "要件§1の入力仕様に沿ってCSVを読み込み、処理対象のhashvinを確認します。セルの出力にはhashvin一覧が表示されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hashvin count: 1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['hv_0001_demo']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# §1: 充電イベントと放置イベントの入力CSVを読み込む\n",
        "SESSIONS_PATH = PROJECT_DIR / 'ev_sessions_1.csv'\n",
        "CHARGE_PATH = PROJECT_DIR / 'ev_charge_to_long_inactive.csv'\n",
        "\n",
        "charge_df = pd.read_csv(CHARGE_PATH)\n",
        "sessions_df = pd.read_csv(SESSIONS_PATH)\n",
        "\n",
        "# hashvinごとの学習を徹底するため一覧を確認（出力: hashvinのリスト）\n",
        "hashvin_list = sorted(charge_df['hashvin'].unique())\n",
        "target_hashvins = hashvin_list  # 一部だけ試す場合はこのリストを編集\n",
        "print(f'hashvin count: {len(hashvin_list)}')\n",
        "hashvin_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 学習・評価実行\n",
        "要件§2〜§8をhashvin単位で順次実行し、\n",
        "esult/<hashvin>/ に評価レポートや予測詳細を保存します。セルの標準出力には進捗とサマリ指標が表示されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Processing hv_0001_demo ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.4.0\n",
            "Python Version:     3.12.10\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.26200\n",
            "CPU Count:          16\n",
            "Memory Avail:       20.49 GB / 31.17 GB (65.7%)\n",
            "Disk Space Avail:   837.01 GB / 930.73 GB (89.9%)\n",
            "===================================================\n",
            "Presets specified: ['medium_quality']\n",
            "Using hyperparameters preset: hyperparameters='default'\n",
            "Beginning AutoGluon training ... Time limit = 300s\n",
            "AutoGluon will save models to \"c:\\workspace\\src\\kaggle\\ml-study\\EV-Battery-Parking-Degradation-Mitigation\\train_ranking\\result\\hv_0001_demo\\autogluon\"\n",
            "Train Data Rows:    28\n",
            "Train Data Columns: 41\n",
            "Tuning Data Rows:    4\n",
            "Tuning Data Columns: 41\n",
            "Label Column:       label\n",
            "Problem Type:       binary\n",
            "Preprocessing data ...\n",
            "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    20973.33 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.02 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 12 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 4): ['daily_bin_00_06_cluster', 'daily_bin_00_06_minutes', 'cand_window_days', 'cand_set_size']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tUnused Original Features (Count: 8): ['charge_lat', 'daily_bin_06_12_minutes', 'soc_start', 'is_return_band', 'weekend_flag', 'cand_events_in_window', 'cand_in_window', 'cand_window_total_events']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', []) : 3 | ['charge_lat', 'daily_bin_06_12_minutes', 'soc_start']\n",
            "\t\t('int', [])   : 5 | ['is_return_band', 'weekend_flag', 'cand_events_in_window', 'cand_in_window', 'cand_window_total_events']\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 17 | ['charge_start_soc', 'charge_lon', 'prev_inactive_lat', 'prev_inactive_lon', 'daily_bin_12_18_minutes', ...]\n",
            "\t\t('int', [])    :  5 | ['inactive_time_minutes', 'session_order', 'dow', 'is_commute_band', 'cand_distance_order']\n",
            "\t\t('object', []) :  7 | ['charge_cluster', 'prev_long_inactive_cluster', 'daily_bin_06_12_cluster', 'daily_bin_12_18_cluster', 'daily_bin_18_24_cluster', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  3 | ['prev_long_inactive_cluster', 'daily_bin_12_18_cluster', 'daily_bin_18_24_cluster']\n",
            "\t\t('float', [])     : 16 | ['charge_start_soc', 'prev_inactive_lat', 'prev_inactive_lon', 'daily_bin_12_18_minutes', 'daily_bin_18_24_minutes', ...]\n",
            "\t\t('int', [])       :  3 | ['session_order', 'dow', 'cand_distance_order']\n",
            "\t\t('int', ['bool']) :  7 | ['charge_cluster', 'charge_lon', 'inactive_time_minutes', 'daily_bin_06_12_cluster', 'is_commute_band', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t29 features in original data used to generate 29 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.07s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}],\n",
            "\t'XGB': [{}],\n",
            "\t'FASTAI': [{}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT ... Training model for up to 299.93s of the 299.93s of remaining time.\n",
            "\tFitting with cpus=8, gpus=0, mem=0.0/20.3 GB\n",
            "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
            "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
            "Fitting model: LightGBM ... Training model for up to 295.84s of the 295.84s of remaining time.\n",
            "\tFitting with cpus=8, gpus=0, mem=0.0/20.3 GB\n",
            "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
            "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
            "Fitting model: RandomForestGini ... Training model for up to 295.75s of the 295.75s of remaining time.\n",
            "\tFitting with cpus=16, gpus=0\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t0.44s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr ... Training model for up to 295.28s of the 295.28s of remaining time.\n",
            "\tFitting with cpus=16, gpus=0\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t0.29s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 294.97s of the 294.97s of remaining time.\n",
            "\tFitting with cpus=8, gpus=0\n",
            "\tWarning: Exception caused CatBoost to fail during training (ImportError)... Skipping this model.\n",
            "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==1.4.0`.\n",
            "Fitting model: ExtraTreesGini ... Training model for up to 294.86s of the 294.86s of remaining time.\n",
            "\tFitting with cpus=16, gpus=0\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t0.29s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr ... Training model for up to 294.54s of the 294.54s of remaining time.\n",
            "\tFitting with cpus=16, gpus=0\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t0.28s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ... Training model for up to 294.23s of the 294.23s of remaining time.\n",
            "\tFitting with cpus=8, gpus=0, mem=0.0/20.3 GB\n",
            "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
            "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.4.0`. \n",
            "Fitting model: XGBoost ... Training model for up to 294.13s of the 294.13s of remaining time.\n",
            "\tFitting with cpus=8, gpus=0\n",
            "\tWarning: Exception caused XGBoost to fail during training (ImportError)... Skipping this model.\n",
            "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.4.0`.\n",
            "Fitting model: NeuralNetTorch ... Training model for up to 294.03s of the 294.03s of remaining time.\n",
            "\tFitting with cpus=8, gpus=0, mem=0.0/20.3 GB\n",
            "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "\t0.5\t = Validation score   (accuracy)\n",
            "\t0.98s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ... Training model for up to 293.04s of the 293.04s of remaining time.\n",
            "\tFitting with cpus=8, gpus=0, mem=0.0/20.3 GB\n",
            "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
            "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.93s of the 292.90s of remaining time.\n",
            "\tEnsemble Weights: {'RandomForestGini': 1.0}\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 7.13s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 137.8 rows/s (4 batch size)\n",
            "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (4 rows).\n",
            "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\workspace\\src\\kaggle\\ml-study\\EV-Battery-Parking-Degradation-Mitigation\\train_ranking\\result\\hv_0001_demo\\autogluon\")\n",
            "These features in provided data are not utilized by the predictor and will be ignored: ['charge_lat', 'daily_bin_00_06_cluster', 'daily_bin_00_06_minutes', 'daily_bin_06_12_minutes', 'soc_start', 'is_return_band', 'weekend_flag', 'cand_window_days', 'cand_events_in_window', 'cand_in_window', 'cand_set_size', 'cand_window_total_events']\n",
            "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:534: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hashvin</th>\n",
              "      <th>feature_columns</th>\n",
              "      <th>status</th>\n",
              "      <th>train_score</th>\n",
              "      <th>feature_importance_path</th>\n",
              "      <th>top1_accuracy</th>\n",
              "      <th>balanced_accuracy</th>\n",
              "      <th>candidate_auc</th>\n",
              "      <th>threshold_metrics</th>\n",
              "      <th>hashvin_summary_path</th>\n",
              "      <th>cluster_summary_path</th>\n",
              "      <th>coverage_mean</th>\n",
              "      <th>candidate_count_mean</th>\n",
              "      <th>model_leaderboard_path</th>\n",
              "      <th>model_leaderboard_top</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hv_0001_demo</td>\n",
              "      <td>[charge_cluster, charge_start_soc, charge_lat,...</td>\n",
              "      <td>trained</td>\n",
              "      <td>{'accuracy': 1.0, 'balanced_accuracy': 1.0, 'm...</td>\n",
              "      <td>c:\\workspace\\src\\kaggle\\ml-study\\EV-Battery-Pa...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>{'0.5': {'coverage': 1.0, 'accuracy': 1.0, 'co...</td>\n",
              "      <td>c:\\workspace\\src\\kaggle\\ml-study\\EV-Battery-Pa...</td>\n",
              "      <td>c:\\workspace\\src\\kaggle\\ml-study\\EV-Battery-Pa...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>c:\\workspace\\src\\kaggle\\ml-study\\EV-Battery-Pa...</td>\n",
              "      <td>[{'model': 'ExtraTreesGini', 'score_test': 1.0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        hashvin                                    feature_columns   status  \\\n",
              "0  hv_0001_demo  [charge_cluster, charge_start_soc, charge_lat,...  trained   \n",
              "\n",
              "                                         train_score  \\\n",
              "0  {'accuracy': 1.0, 'balanced_accuracy': 1.0, 'm...   \n",
              "\n",
              "                             feature_importance_path  top1_accuracy  \\\n",
              "0  c:\\workspace\\src\\kaggle\\ml-study\\EV-Battery-Pa...            1.0   \n",
              "\n",
              "   balanced_accuracy  candidate_auc  \\\n",
              "0                1.0            1.0   \n",
              "\n",
              "                                   threshold_metrics  \\\n",
              "0  {'0.5': {'coverage': 1.0, 'accuracy': 1.0, 'co...   \n",
              "\n",
              "                                hashvin_summary_path  \\\n",
              "0  c:\\workspace\\src\\kaggle\\ml-study\\EV-Battery-Pa...   \n",
              "\n",
              "                                cluster_summary_path  coverage_mean  \\\n",
              "0  c:\\workspace\\src\\kaggle\\ml-study\\EV-Battery-Pa...            1.0   \n",
              "\n",
              "   candidate_count_mean                             model_leaderboard_path  \\\n",
              "0                   2.0  c:\\workspace\\src\\kaggle\\ml-study\\EV-Battery-Pa...   \n",
              "\n",
              "                               model_leaderboard_top  \n",
              "0  [{'model': 'ExtraTreesGini', 'score_test': 1.0...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# §2〜§8: hashvinごとの前処理・学習・評価・成果物保存フロー\n",
        "RESULT_ROOT = PROJECT_DIR / 'result'\n",
        "RESULT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "metrics_summary = []\n",
        "for hashvin in target_hashvins:\n",
        "    print(f'=== Processing {hashvin} ===')  # 進捗ログ\n",
        "    charge_subset = charge_df[charge_df['hashvin'] == hashvin].copy()\n",
        "    sessions_subset = sessions_df[sessions_df['hashvin'] == hashvin].copy()\n",
        "\n",
        "    processor = HashvinProcessor(hashvin, charge_subset, sessions_subset, pipeline_config)\n",
        "    result = processor.build_features()  # §2〜§5: 特徴生成とHEAD抽出\n",
        "\n",
        "    hashvin_dir = RESULT_ROOT / hashvin\n",
        "    metrics = train_and_evaluate(\n",
        "        result,\n",
        "        hashvin_dir,\n",
        "        enable_station_type=pipeline_config.enable_station_type,\n",
        "        autogluon_presets=autogluon_presets,\n",
        "        autogluon_time_limit=autogluon_time_limit,\n",
        "        eval_thresholds=eval_thresholds,\n",
        "    )  # §6〜§8: AutoGluon学習と評価レポート出力\n",
        "    metrics_summary.append(metrics)\n",
        "\n",
        "if metrics_summary:\n",
        "    summary_path_json = RESULT_ROOT / 'metrics_summary.json'\n",
        "    summary_path_csv = RESULT_ROOT / 'metrics_summary.csv'\n",
        "    summary_path_json.write_text(json.dumps(metrics_summary, indent=2, ensure_ascii=False), encoding='utf-8')\n",
        "    pd.DataFrame(metrics_summary).to_csv(summary_path_csv, index=False)\n",
        "    display(pd.DataFrame(metrics_summary))  # Jupyter上で指標を可視化\n",
        "else:\n",
        "    print('No hashvin processed. Check target_hashvins list.')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
