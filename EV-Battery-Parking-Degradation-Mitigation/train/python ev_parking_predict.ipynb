{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9a8921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.12.10\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          16\n",
      "Memory Avail:       19.67 GB / 31.17 GB (63.1%)\n",
      "Disk Space Avail:   833.37 GB / 930.73 GB (89.5%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 900s of the 3600s of remaining time (25%).\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1444: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ImportError('ray is required to train folds in parallel for TabularPredictor or HPO for MultiModalPredictor. A quick tip is to install via `pip install \"ray>=2.10.0,<2.45.0\"`')\n",
      "  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n",
      "\t\tContext path: \"c:\\workspace\\src\\kaggle\\ml-study\\EV-Battery-Parking-Degradation-Mitigation\\train\\outputs\\predictor_time_raw_global\\ds_sub_fit\\sub_fit_ho\"\n",
      "Running DyStack sub-fit ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] CSV未指定のためサンプルデータを生成します。\n",
      "\n",
      "===== Experiment: time_raw (raw) =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 900s\n",
      "AutoGluon will save models to \"c:\\workspace\\src\\kaggle\\ml-study\\EV-Battery-Parking-Degradation-Mitigation\\train\\outputs\\predictor_time_raw_global\\ds_sub_fit\\sub_fit_ho\"\n",
      "Train Data Rows:    415\n",
      "Train Data Columns: 3\n",
      "Label Column:       inactive_cluster_id\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 6\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    20136.99 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.05 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('datetime', []) : 1 | ['charge_start_time']\n",
      "\t\t('object', [])   : 2 | ['hashvin', 'charge_cluster_id']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])             : 2 | ['hashvin', 'charge_cluster_id']\n",
      "\t\t('int', ['datetime_as_int']) : 4 | ['charge_start_time', 'charge_start_time.month', 'charge_start_time.day', 'charge_start_time.dayofweek']\n",
      "\t0.0s = Fit runtime\n",
      "\t3 features in original data used to generate 6 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.04s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 599.82s of the 899.95s of remaining time.\n",
      "Will use sequential fold fitting strategy because import of ray failed. Reason: ray is required to train folds in parallel for TabularPredictor or HPO for MultiModalPredictor. A quick tip is to install via `pip install \"ray>=2.10.0,<2.45.0\"`\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=0)\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.4.0`. \n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 596.09s of the 896.22s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=0)\n",
      "\tWarning: Exception caused LightGBMXT_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 595.97s of the 896.10s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=0)\n",
      "\tWarning: Exception caused LightGBM_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 595.85s of the 895.98s of remaining time.\n",
      "\t0.2096\t = Validation score   (accuracy)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 595.24s of the 895.37s of remaining time.\n",
      "\t0.212\t = Validation score   (accuracy)\n",
      "\t0.37s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 594.79s of the 894.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=0)\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==1.4.0`.\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 594.67s of the 894.80s of remaining time.\n",
      "\t0.212\t = Validation score   (accuracy)\n",
      "\t0.37s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 594.22s of the 894.35s of remaining time.\n",
      "\t0.1976\t = Validation score   (accuracy)\n",
      "\t0.38s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 593.76s of the 893.89s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=0)\n",
      "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.4.0`.\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 593.63s of the 893.76s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=0)\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t0.3398\t = Validation score   (accuracy)\n",
      "\t4.88s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 588.68s of the 888.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=0)\n",
      "\tWarning: Exception caused LightGBMLarge_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 588.51s of the 888.64s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=0)\n",
      "\tWarning: Exception caused CatBoost_r177_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==1.4.0`.\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 588.36s of the 888.49s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=0)\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t0.3446\t = Validation score   (accuracy)\n",
      "\t4.76s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 583.53s of the 883.66s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=0)\n",
      "\tWarning: Exception caused LightGBM_r131_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 583.37s of the 883.50s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=0)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r191_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.4.0`. \n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 583.22s of the 883.35s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=0)\n",
      "\tWarning: Exception caused CatBoost_r9_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==1.4.0`.\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 583.06s of the 883.19s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=0)\n",
      "\tWarning: Exception caused LightGBM_r96_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 582.92s of the 883.05s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=0)\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t0.3566\t = Validation score   (accuracy)\n",
      "\t4.2s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 578.65s of the 878.78s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=0)\n",
      "\tWarning: Exception caused XGBoost_r33_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.4.0`.\n",
      "Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 578.50s of the 878.63s of remaining time.\n",
      "\t0.2169\t = Validation score   (accuracy)\n",
      "\t0.41s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 578.01s of the 878.14s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=0)\n",
      "\tWarning: Exception caused CatBoost_r137_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==1.4.0`.\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 577.87s of the 878.00s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=0)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r102_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.4.0`. \n",
      "Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 577.72s of the 877.85s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=0)\n",
      "\tWarning: Exception caused CatBoost_r13_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==1.4.0`.\n",
      "Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 577.57s of the 877.70s of remaining time.\n",
      "\t0.212\t = Validation score   (accuracy)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 576.89s of the 877.02s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=0)\n",
      "\tWarning: Exception caused LightGBM_r188_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 576.73s of the 876.86s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=0)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r145_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.4.0`. \n",
      "Fitting model: XGBoost_r89_BAG_L1 ... Training model for up to 576.59s of the 876.72s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=0)\n",
      "\tWarning: Exception caused XGBoost_r89_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.4.0`.\n",
      "Fitting model: NeuralNetTorch_r30_BAG_L1 ... Training model for up to 576.44s of the 876.57s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=8, gpus=0)\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 368\u001b[39m\n\u001b[32m    366\u001b[39m \u001b[38;5;66;03m# Jupyterからの不要な引数を除去\u001b[39;00m\n\u001b[32m    367\u001b[39m sys.argv = [sys.argv[\u001b[32m0\u001b[39m]]\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 316\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    313\u001b[39m features_global = base_feats + time_feats\n\u001b[32m    315\u001b[39m \u001b[38;5;66;03m# --- 共通モデル（hashvinを含める）---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m global_pred, global_metrics = \u001b[43mtrain_eval_global\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_global\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minactive_cluster_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mout\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m summary_rows.append({\n\u001b[32m    320\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msetting\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGLOBAL_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    321\u001b[39m     **global_metrics\n\u001b[32m    322\u001b[39m })\n\u001b[32m    324\u001b[39m \u001b[38;5;66;03m# --- 個別モデル（hashvinごと）---\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# 個別モデルでは特徴からhashvinは外す（固定値のため）\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mtrain_eval_global\u001b[39m\u001b[34m(df, feature_cols, label, exp_name, out_dir)\u001b[39m\n\u001b[32m    109\u001b[39m use_cols = feature_cols + [label]\n\u001b[32m    110\u001b[39m data = TabularDataset(df[use_cols])\n\u001b[32m    111\u001b[39m predictor = \u001b[43mTabularPredictor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproblem_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmulticlass\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maccuracy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpredictor_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mexp_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_global\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpresets\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbest_quality\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    118\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# ホールドアウトがないので同データでの再評価 + 予測確率\u001b[39;00m\n\u001b[32m    120\u001b[39m y_true = data[label].astype(\u001b[38;5;28mstr\u001b[39m).tolist()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\core\\utils\\decorators.py:31\u001b[39m, in \u001b[36munpack.<locals>._unpack_inner.<locals>._call\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(f)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call\u001b[39m(*args, **kwargs):\n\u001b[32m     30\u001b[39m     gargs, gkwargs = g(*other_args, *args, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1344\u001b[39m, in \u001b[36mTabularPredictor.fit\u001b[39m\u001b[34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, fit_full_last_level_weighted_ensemble, full_weighted_ensemble_additionally, dynamic_stacking, calibrate_decision_threshold, num_cpus, num_gpus, fit_strategy, memory_limit, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dynamic_stacking:\n\u001b[32m   1339\u001b[39m     logger.log(\n\u001b[32m   1340\u001b[39m         \u001b[32m20\u001b[39m,\n\u001b[32m   1341\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDyStack is enabled (dynamic_stacking=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdynamic_stacking\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m). \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1342\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1343\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1344\u001b[39m     num_stack_levels, time_limit = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dynamic_stacking\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mds_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1345\u001b[39m     logger.info(\n\u001b[32m   1346\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting main fit with num_stack_levels=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_stack_levels\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1347\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33mFor future fit calls on this dataset, you can skip DyStack to save time: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1348\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`predictor.fit(..., dynamic_stacking=False, num_stack_levels=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_stack_levels\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1349\u001b[39m     )\n\u001b[32m   1351\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (time_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (time_limit <= \u001b[32m0\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1444\u001b[39m, in \u001b[36mTabularPredictor._dynamic_stacking\u001b[39m\u001b[34m(self, ag_fit_kwargs, ag_post_fit_kwargs, validation_procedure, detection_time_frac, holdout_frac, n_folds, n_repeats, memory_safe_fits, clean_up_fits, enable_ray_logging, enable_callbacks, holdout_data)\u001b[39m\n\u001b[32m   1441\u001b[39m         _, holdout_data, _, _ = \u001b[38;5;28mself\u001b[39m._validate_fit_data(train_data=X, tuning_data=holdout_data)\n\u001b[32m   1442\u001b[39m         ds_fit_kwargs[\u001b[33m\"\u001b[39m\u001b[33mds_fit_context\u001b[39m\u001b[33m\"\u001b[39m] = os.path.join(ds_fit_context, \u001b[33m\"\u001b[39m\u001b[33msub_fit_custom_ho\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1444\u001b[39m     stacked_overfitting = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sub_fit_memory_save_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1446\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1447\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1448\u001b[39m \u001b[43m        \u001b[49m\u001b[43mds_fit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[43m        \u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_ag_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1450\u001b[39m \u001b[43m        \u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_ag_post_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1451\u001b[39m \u001b[43m        \u001b[49m\u001b[43mholdout_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mholdout_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1452\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1453\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1454\u001b[39m     \u001b[38;5;66;03m# Holdout is false, use (repeated) cross-validation\u001b[39;00m\n\u001b[32m   1455\u001b[39m     is_stratified = \u001b[38;5;28mself\u001b[39m.problem_type \u001b[38;5;129;01min\u001b[39;00m [BINARY, MULTICLASS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1648\u001b[39m, in \u001b[36mTabularPredictor._sub_fit_memory_save_wrapper\u001b[39m\u001b[34m(self, train_data, time_limit, time_start, ds_fit_kwargs, ag_fit_kwargs, ag_post_fit_kwargs, holdout_data)\u001b[39m\n\u001b[32m   1645\u001b[39m     normal_fit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1647\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m normal_fit:\n\u001b[32m-> \u001b[39m\u001b[32m1648\u001b[39m     stacked_overfitting, ho_leaderboard, exception = \u001b[43m_dystack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1650\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mds_fit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mholdout_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mholdout_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1658\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1659\u001b[39m     logger.log(\u001b[32m40\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWarning: Exception encountered during DyStack sub-fit:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexception\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:5851\u001b[39m, in \u001b[36m_dystack\u001b[39m\u001b[34m(predictor, train_data, time_limit, ds_fit_kwargs, ag_fit_kwargs, ag_post_fit_kwargs, holdout_data)\u001b[39m\n\u001b[32m   5849\u001b[39m logger.log(\u001b[32m20\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning DyStack sub-fit ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5850\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5851\u001b[39m     \u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5852\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   5853\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1369\u001b[39m, in \u001b[36mTabularPredictor._fit\u001b[39m\u001b[34m(self, ag_fit_kwargs, ag_post_fit_kwargs)\u001b[39m\n\u001b[32m   1367\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, ag_fit_kwargs: \u001b[38;5;28mdict\u001b[39m, ag_post_fit_kwargs: \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m   1368\u001b[39m     \u001b[38;5;28mself\u001b[39m.save(silent=\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Save predictor to disk to enable prediction and training after interrupt\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1369\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_learner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1370\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_post_fit_vars()\n\u001b[32m   1371\u001b[39m     \u001b[38;5;28mself\u001b[39m._post_fit(**ag_post_fit_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:159\u001b[39m, in \u001b[36mAbstractTabularLearner.fit\u001b[39m\u001b[34m(self, X, X_val, **kwargs)\u001b[39m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mLearner is already fit.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    158\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_fit_input(X=X, X_val=X_val, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\tabular\\learner\\default_learner.py:133\u001b[39m, in \u001b[36mDefaultLearner._fit\u001b[39m\u001b[34m(self, X, X_val, X_test, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, infer_limit, infer_limit_batch_size, verbosity, raise_on_model_failure, **trainer_fit_kwargs)\u001b[39m\n\u001b[32m    130\u001b[39m     \u001b[38;5;28mself\u001b[39m.eval_metric = trainer.eval_metric\n\u001b[32m    132\u001b[39m \u001b[38;5;28mself\u001b[39m.save()\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mholdout_frac\u001b[49m\u001b[43m=\u001b[49m\u001b[43mholdout_frac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_limit_trainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrainer_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28mself\u001b[39m.save_trainer(trainer=trainer)\n\u001b[32m    149\u001b[39m time_end = time.time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\tabular\\trainer\\auto_trainer.py:135\u001b[39m, in \u001b[36mAutoTrainer.fit\u001b[39m\u001b[34m(self, X, y, hyperparameters, X_val, y_val, X_test, y_test, X_unlabeled, holdout_frac, num_stack_levels, core_kwargs, aux_kwargs, time_limit, infer_limit, infer_limit_batch_size, use_bag_holdout, groups, callbacks, **kwargs)\u001b[39m\n\u001b[32m    132\u001b[39m log_str += \u001b[33m\"\u001b[39m\u001b[33m}\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m logger.log(\u001b[32m20\u001b[39m, log_str)\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_multi_and_ensemble\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_stack_levels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_stack_levels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcore_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcore_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43maux_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43maux_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m    \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py:3313\u001b[39m, in \u001b[36mAbstractTabularTrainer._train_multi_and_ensemble\u001b[39m\u001b[34m(self, X, y, X_val, y_val, X_test, y_test, hyperparameters, X_unlabeled, num_stack_levels, time_limit, groups, **kwargs)\u001b[39m\n\u001b[32m   3311\u001b[39m     \u001b[38;5;28mself\u001b[39m._num_rows_test = \u001b[38;5;28mlen\u001b[39m(X_test)\n\u001b[32m   3312\u001b[39m \u001b[38;5;28mself\u001b[39m._num_cols_train = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(X.columns))\n\u001b[32m-> \u001b[39m\u001b[32m3313\u001b[39m model_names_fit = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_multi_levels\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3314\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3315\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3316\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3317\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3318\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3319\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3320\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel_start\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel_end\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_stack_levels\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3325\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3326\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.get_model_names()) == \u001b[32m0\u001b[39m:\n\u001b[32m   3328\u001b[39m     \u001b[38;5;66;03m# TODO v1.0: Add toggle to raise exception if no models trained\u001b[39;00m\n\u001b[32m   3329\u001b[39m     logger.log(\u001b[32m30\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mWarning: AutoGluon did not successfully train any models\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py:506\u001b[39m, in \u001b[36mAbstractTabularTrainer.train_multi_levels\u001b[39m\u001b[34m(self, X, y, hyperparameters, X_val, y_val, X_test, y_test, X_unlabeled, base_model_names, core_kwargs, aux_kwargs, level_start, level_end, time_limit, name_suffix, relative_stack, level_time_modifier, infer_limit, infer_limit_batch_size, callbacks)\u001b[39m\n\u001b[32m    504\u001b[39m         core_kwargs_level[\u001b[33m\"\u001b[39m\u001b[33mtime_limit\u001b[39m\u001b[33m\"\u001b[39m] = core_kwargs_level.get(\u001b[33m\"\u001b[39m\u001b[33mtime_limit\u001b[39m\u001b[33m\"\u001b[39m, time_limit_core)\n\u001b[32m    505\u001b[39m         aux_kwargs_level[\u001b[33m\"\u001b[39m\u001b[33mtime_limit\u001b[39m\u001b[33m\"\u001b[39m] = aux_kwargs_level.get(\u001b[33m\"\u001b[39m\u001b[33mtime_limit\u001b[39m\u001b[33m\"\u001b[39m, time_limit_aux)\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m     base_model_names, aux_models = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstack_new_level\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbase_model_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_model_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcore_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcore_kwargs_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43maux_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43maux_kwargs_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname_suffix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname_suffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m        \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfull_weighted_ensemble\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_weighted_ensemble\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_full_weighted_ensemble\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_full_weighted_ensemble\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    525\u001b[39m     model_names_fit += base_model_names + aux_models\n\u001b[32m    526\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.model_best \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m infer_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model_names_fit) != \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py:706\u001b[39m, in \u001b[36mAbstractTabularTrainer.stack_new_level\u001b[39m\u001b[34m(self, X, y, models, X_val, y_val, X_test, y_test, X_unlabeled, level, base_model_names, core_kwargs, aux_kwargs, name_suffix, infer_limit, infer_limit_batch_size, full_weighted_ensemble, additional_full_weighted_ensemble)\u001b[39m\n\u001b[32m    704\u001b[39m     core_kwargs[\u001b[33m\"\u001b[39m\u001b[33mname_suffix\u001b[39m\u001b[33m\"\u001b[39m] = core_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mname_suffix\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) + name_suffix\n\u001b[32m    705\u001b[39m     aux_kwargs[\u001b[33m\"\u001b[39m\u001b[33mname_suffix\u001b[39m\u001b[33m\"\u001b[39m] = aux_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mname_suffix\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) + name_suffix\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m core_models = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstack_new_level_core\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m    \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m    \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_model_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_model_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcore_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    722\u001b[39m aux_models = []\n\u001b[32m    723\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m full_weighted_ensemble:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py:857\u001b[39m, in \u001b[36mAbstractTabularTrainer.stack_new_level_core\u001b[39m\u001b[34m(self, X, y, models, X_val, y_val, X_test, y_test, X_unlabeled, level, base_model_names, fit_strategy, stack_name, ag_args, ag_args_fit, ag_args_ensemble, included_model_types, excluded_model_types, ensemble_type, name_suffix, get_models_func, refit_full, infer_limit, infer_limit_batch_size, **kwargs)\u001b[39m\n\u001b[32m    851\u001b[39m fit_kwargs = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m    852\u001b[39m     num_classes=\u001b[38;5;28mself\u001b[39m.num_classes,\n\u001b[32m    853\u001b[39m     feature_metadata=feature_metadata,\n\u001b[32m    854\u001b[39m )\n\u001b[32m    856\u001b[39m \u001b[38;5;66;03m# FIXME: TODO: v0.1 X_unlabeled isn't cached so it won't be available during refit_full or fit_extra.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m857\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_multi\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstack_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstack_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfit_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfit_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py:3245\u001b[39m, in \u001b[36mAbstractTabularTrainer._train_multi\u001b[39m\u001b[34m(self, X, y, models, hyperparameter_tune_kwargs, feature_prune_kwargs, k_fold, n_repeats, n_repeat_start, time_limit, delay_bag_sets, **kwargs)\u001b[39m\n\u001b[32m   3243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_repeat_start == \u001b[32m0\u001b[39m:\n\u001b[32m   3244\u001b[39m     time_start = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3245\u001b[39m     model_names_trained = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_multi_initial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3247\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk_fold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk_fold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_repeats_initial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_prune_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_prune_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3254\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3256\u001b[39m     n_repeat_start = n_repeats_initial\n\u001b[32m   3257\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m time_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py:2840\u001b[39m, in \u001b[36mAbstractTabularTrainer._train_multi_initial\u001b[39m\u001b[34m(self, X, y, models, k_fold, n_repeats, hyperparameter_tune_kwargs, time_limit, feature_prune_kwargs, **kwargs)\u001b[39m\n\u001b[32m   2838\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2839\u001b[39m     time_ratio = hpo_time_ratio \u001b[38;5;28;01mif\u001b[39;00m hpo_enabled \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2840\u001b[39m     models = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_multi_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk_fold_start\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk_fold_end\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk_fold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_repeat_start\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2848\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtime_split\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2849\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtime_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2850\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2851\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2853\u001b[39m multi_fold_time_elapsed = time.time() - multi_fold_time_start\n\u001b[32m   2854\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m time_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py:2997\u001b[39m, in \u001b[36mAbstractTabularTrainer._train_multi_fold\u001b[39m\u001b[34m(self, X, y, models, time_limit, time_split, time_ratio, hyperparameter_tune_kwargs, fit_strategy, **kwargs)\u001b[39m\n\u001b[32m   2994\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._callback_early_stop:\n\u001b[32m   2995\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m models_valid\n\u001b[32m-> \u001b[39m\u001b[32m2997\u001b[39m         models_valid += \u001b[43m_detached_train_multi_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2998\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_self\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2999\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3000\u001b[39m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3001\u001b[39m \u001b[43m            \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3002\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3003\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_split\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3004\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3005\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_limit_model_split\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_limit_model_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3006\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3007\u001b[39m \u001b[43m            \u001b[49m\u001b[43mis_ray_worker\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3008\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3009\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3010\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m fit_strategy == \u001b[33m\"\u001b[39m\u001b[33mparallel\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3011\u001b[39m     models_valid = \u001b[38;5;28mself\u001b[39m._train_multi_fold_parallel(\n\u001b[32m   3012\u001b[39m         X=X,\n\u001b[32m   3013\u001b[39m         y=y,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3020\u001b[39m         **kwargs,\n\u001b[32m   3021\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py:4588\u001b[39m, in \u001b[36m_detached_train_multi_fold\u001b[39m\u001b[34m(_self, model, X, y, time_split, time_start, time_limit, time_limit_model_split, hyperparameter_tune_kwargs, is_ray_worker, kwargs)\u001b[39m\n\u001b[32m   4585\u001b[39m         time_start_model=time.time()\n\u001b[32m   4586\u001b[39m         time_left=time_limit-(time_start_model-time_start)\n\u001b[32m-> \u001b[39m\u001b[32m4588\u001b[39m model_name_trained_lst = \u001b[43m_self\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_train_single_full\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4589\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4590\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_left\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4593\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhyperparameter_tune_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhyperparameter_tune_kwargs_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4594\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_ray_worker\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_ray_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4595\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   4596\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4598\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _self.low_memory:\n\u001b[32m   4599\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py:2613\u001b[39m, in \u001b[36mAbstractTabularTrainer._train_single_full\u001b[39m\u001b[34m(self, X, y, model, X_unlabeled, X_val, y_val, X_test, y_test, X_pseudo, y_pseudo, hyperparameter_tune_kwargs, stack_name, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, level, time_limit, fit_kwargs, compute_score, total_resources, errors, errors_ignore, errors_raise, is_ray_worker, **kwargs)\u001b[39m\n\u001b[32m   2609\u001b[39m         bagged_model_fit_kwargs = \u001b[38;5;28mself\u001b[39m._get_bagged_model_fit_kwargs(\n\u001b[32m   2610\u001b[39m             k_fold=k_fold, k_fold_start=k_fold_start, k_fold_end=k_fold_end, n_repeats=n_repeats, n_repeat_start=n_repeat_start\n\u001b[32m   2611\u001b[39m         )\n\u001b[32m   2612\u001b[39m         model_fit_kwargs.update(bagged_model_fit_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2613\u001b[39m     model_names_trained = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_and_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2614\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2615\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2616\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2617\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2618\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2619\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2620\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstack_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstack_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompute_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_resources\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_resources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2626\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2627\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors_ignore\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors_ignore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2628\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors_raise\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors_raise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_ray_worker\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_ray_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2630\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2631\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2632\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks \u001b[38;5;129;01mand\u001b[39;00m check_callbacks:\n\u001b[32m   2633\u001b[39m     \u001b[38;5;28mself\u001b[39m._callbacks_after_fit(model_names=model_names_trained, stack_name=stack_name, level=level)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py:2171\u001b[39m, in \u001b[36mAbstractTabularTrainer._train_and_save\u001b[39m\u001b[34m(self, X, y, model, X_val, y_val, X_test, y_test, X_pseudo, y_pseudo, time_limit, stack_name, level, compute_score, total_resources, errors, errors_ignore, errors_raise, is_ray_worker, **model_fit_kwargs)\u001b[39m\n\u001b[32m   2169\u001b[39m exception = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2170\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2171\u001b[39m     model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2173\u001b[39m     fit_end_time = time.time()\n\u001b[32m   2174\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.weight_evaluation:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py:2055\u001b[39m, in \u001b[36mAbstractTabularTrainer._train_single\u001b[39m\u001b[34m(self, X, y, model, X_val, y_val, X_test, y_test, total_resources, **model_fit_kwargs)\u001b[39m\n\u001b[32m   2039\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_train_single\u001b[39m(\n\u001b[32m   2040\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2041\u001b[39m     X: pd.DataFrame,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2049\u001b[39m     **model_fit_kwargs,\n\u001b[32m   2050\u001b[39m ) -> AbstractModel:\n\u001b[32m   2051\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2052\u001b[39m \u001b[33;03m    Trains model but does not add the trained model to this Trainer.\u001b[39;00m\n\u001b[32m   2053\u001b[39m \u001b[33;03m    Returns trained model object.\u001b[39;00m\n\u001b[32m   2054\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2055\u001b[39m     model = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_resources\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_resources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2056\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py:1068\u001b[39m, in \u001b[36mAbstractModel.fit\u001b[39m\u001b[34m(self, log_resources, **kwargs)\u001b[39m\n\u001b[32m   1066\u001b[39m         msg += msg_mem\n\u001b[32m   1067\u001b[39m     logger.log(\u001b[32m20\u001b[39m, msg)\n\u001b[32m-> \u001b[39m\u001b[32m1068\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1070\u001b[39m     out = \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py:270\u001b[39m, in \u001b[36mStackerEnsembleModel._fit\u001b[39m\u001b[34m(self, X, y, compute_base_preds, time_limit, **kwargs)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m time_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    269\u001b[39m     time_limit = time_limit - (time.time() - start_time)\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py:389\u001b[39m, in \u001b[36mBaggedEnsembleModel._fit\u001b[39m\u001b[34m(self, X, y, X_val, y_val, X_pseudo, y_pseudo, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, groups, _skip_oof, **kwargs)\u001b[39m\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# Reserve time for final refit model\u001b[39;00m\n\u001b[32m    388\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mtime_limit\u001b[39m\u001b[33m\"\u001b[39m] = kwargs[\u001b[33m\"\u001b[39m\u001b[33mtime_limit\u001b[39m\u001b[33m\"\u001b[39m] * folds_to_fit / (folds_to_fit + \u001b[32m1.2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_folds\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_pseudo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_pseudo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_pseudo\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_pseudo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk_fold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk_fold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk_fold_start\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk_fold_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk_fold_end\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk_fold_end\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_repeat_start\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_repeat_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_folds\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_bag_folds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[38;5;66;03m# FIXME: Cleanup self\u001b[39;00m\n\u001b[32m    405\u001b[39m \u001b[38;5;66;03m# FIXME: Support `can_refit_full=False` models\u001b[39;00m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m refit_folds:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py:868\u001b[39m, in \u001b[36mBaggedEnsembleModel._fit_folds\u001b[39m\u001b[34m(self, X, y, model_base, X_pseudo, y_pseudo, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, time_limit, sample_weight, save_folds, groups, num_cpus, num_gpus, **kwargs)\u001b[39m\n\u001b[32m    866\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fold_fit_args \u001b[38;5;129;01min\u001b[39;00m fold_fit_args_list:\n\u001b[32m    867\u001b[39m     fold_fitting_strategy.schedule_fold_model_fit(**fold_fit_args)\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m \u001b[43mfold_fitting_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mafter_all_folds_scheduled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[38;5;66;03m# Do this to maintain model name order based on kfold split regardless of which model finished first in parallel mode\u001b[39;00m\n\u001b[32m    871\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fold_fit_args \u001b[38;5;129;01min\u001b[39;00m fold_fit_args_list:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py:350\u001b[39m, in \u001b[36mSequentialLocalFoldFittingStrategy.after_all_folds_scheduled\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mafter_all_folds_scheduled\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    349\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m job \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jobs:\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_fold_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py:355\u001b[39m, in \u001b[36mSequentialLocalFoldFittingStrategy._fit_fold_model\u001b[39m\u001b[34m(self, fold_ctx)\u001b[39m\n\u001b[32m    353\u001b[39m time_start_fold = time.time()\n\u001b[32m    354\u001b[39m time_limit_fold = \u001b[38;5;28mself\u001b[39m._get_fold_time_limit(fold_ctx)\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m fold_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_start_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_limit_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_base_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m fold_model, pred_proba = \u001b[38;5;28mself\u001b[39m._predict_oof(fold_model, fold_ctx)\n\u001b[32m    357\u001b[39m \u001b[38;5;28mself\u001b[39m._update_bagged_ensemble(fold_model, pred_proba, fold_ctx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py:391\u001b[39m, in \u001b[36mSequentialLocalFoldFittingStrategy._fit\u001b[39m\u001b[34m(self, model_base, time_start_fold, time_limit_fold, fold_ctx, kwargs)\u001b[39m\n\u001b[32m    389\u001b[39m     num_cpus = \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_cpus, \u001b[38;5;28mself\u001b[39m.user_resources_per_job.get(\u001b[33m\"\u001b[39m\u001b[33mnum_cpus\u001b[39m\u001b[33m\"\u001b[39m, math.inf))\n\u001b[32m    390\u001b[39m     num_gpus = \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_gpus, \u001b[38;5;28mself\u001b[39m.user_resources_per_job.get(\u001b[33m\"\u001b[39m\u001b[33mnum_gpus\u001b[39m\u001b[33m\"\u001b[39m, math.inf))\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m \u001b[43mfold_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_val_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_val_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_limit_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_cpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_fold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    392\u001b[39m fold_model.fit_time = time.time() - time_start_fold\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m fold_model\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py:1068\u001b[39m, in \u001b[36mAbstractModel.fit\u001b[39m\u001b[34m(self, log_resources, **kwargs)\u001b[39m\n\u001b[32m   1066\u001b[39m         msg += msg_mem\n\u001b[32m   1067\u001b[39m     logger.log(\u001b[32m20\u001b[39m, msg)\n\u001b[32m-> \u001b[39m\u001b[32m1068\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1070\u001b[39m     out = \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py:250\u001b[39m, in \u001b[36mTabularNeuralNetTorchModel._fit\u001b[39m\u001b[34m(self, X, y, X_val, y_val, X_test, y_test, time_limit, sample_weight, num_cpus, num_gpus, reporter, verbosity, **kwargs)\u001b[39m\n\u001b[32m    247\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m TimeLimitExceeded\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# train network\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_net\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreporter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreporter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py:386\u001b[39m, in \u001b[36mTabularNeuralNetTorchModel._train_net\u001b[39m\u001b[34m(self, train_dataset, loss_kwargs, batch_size, num_epochs, epochs_wo_improve, val_dataset, test_dataset, time_limit, reporter, verbosity)\u001b[39m\n\u001b[32m    383\u001b[39m total_train_size = \u001b[32m0.0\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, data_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[32m    385\u001b[39m     \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mloss_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    387\u001b[39m     total_train_loss += loss.item()\n\u001b[32m    388\u001b[39m     total_train_size += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\torch_network_modules.py:226\u001b[39m, in \u001b[36mEmbedNet.compute_loss\u001b[39m\u001b[34m(self, data_batch, loss_function, gamma)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_batch, loss_function=\u001b[38;5;28;01mNone\u001b[39;00m, gamma=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    224\u001b[39m     \u001b[38;5;66;03m# train mode\u001b[39;00m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28mself\u001b[39m.train()\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m     predict_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m     target_data = data_batch[-\u001b[32m1\u001b[39m].to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.problem_type \u001b[38;5;129;01min\u001b[39;00m [BINARY, MULTICLASS]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\torch_network_modules.py:163\u001b[39m, in \u001b[36mEmbedNet.forward\u001b[39m\u001b[34m(self, data_batch)\u001b[39m\n\u001b[32m    161\u001b[39m     embed_data = data_batch[input_offset]\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.embed_blocks)):\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m         input_data.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_blocks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_data) > \u001b[32m1\u001b[39m:\n\u001b[32m    166\u001b[39m     input_data = torch.cat(input_data, dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:192\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2546\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2540\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2541\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2542\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2543\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2544\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2545\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2546\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# filename: ev_parking_predict.py\n",
    "# -*- coding: utf-8 -*-\n",
    "# 要件:\n",
    "# - 共通モデル(単一) + 個別モデル(hashvinごと) の学習/評価を実行\n",
    "# - 特徴量に charge_cluster_id を必須で使用\n",
    "# - charge_start_time をそのまま(datetime)投入 / 周期特徴へ加工 の両方を実験\n",
    "# - 多クラス評価: accuracy / top-3 accuracy / log_loss / macro_f1 + 混同行列保存\n",
    "# - CSV未指定ならサンプルデータを自動生成（列名は本案件準拠で上書き）\n",
    "#\n",
    "# 依存: pip install autogluon.tabular==1.* scikit-learn pandas numpy matplotlib\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, log_loss, confusion_matrix\n",
    ")\n",
    "\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# ユーティリティ\n",
    "# ---------------------------\n",
    "def ensure_categorical(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(str)\n",
    "    return df\n",
    "\n",
    "def make_time_features(df, dt_col, mode=\"raw\"):\n",
    "    \"\"\"mode:\n",
    "        - 'raw': dtをdatetime型で渡す (AutoGluonが自動派生)\n",
    "        - 'engineered': 曜日/時刻/周期sin-cosを明示的に追加\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    if dt_col not in out.columns:\n",
    "        raise ValueError(f\"{dt_col} がありません\")\n",
    "    # to datetime\n",
    "    out[dt_col] = pd.to_datetime(out[dt_col])\n",
    "\n",
    "    if mode == \"raw\":\n",
    "        # そのまま。AutoGluonが年月日・曜日・時刻など派生してくれる\n",
    "        return out, [dt_col]\n",
    "\n",
    "    elif mode == \"engineered\":\n",
    "        out[\"weekday\"] = out[dt_col].dt.weekday            # 0=Mon..6=Sun\n",
    "        out[\"hour\"] = out[dt_col].dt.hour\n",
    "\n",
    "        # 周期性 (時間 24h / 曜日 7d)\n",
    "        out[\"hour_sin\"] = np.sin(2*np.pi*out[\"hour\"]/24.0)\n",
    "        out[\"hour_cos\"] = np.cos(2*np.pi*out[\"hour\"]/24.0)\n",
    "        out[\"wday_sin\"] = np.sin(2*np.pi*out[\"weekday\"]/7.0)\n",
    "        out[\"wday_cos\"] = np.cos(2*np.pi*out[\"weekday\"]/7.0)\n",
    "\n",
    "        feat_list = [\"weekday\", \"hour\", \"hour_sin\", \"hour_cos\", \"wday_sin\", \"wday_cos\"]\n",
    "        return out, feat_list\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"modeは 'raw' か 'engineered' を指定してください\")\n",
    "\n",
    "\n",
    "def top_k_accuracy(y_true, proba_df, k=3):\n",
    "    classes = list(proba_df.columns)\n",
    "    idx_map = {c:i for i,c in enumerate(classes)}\n",
    "    true_idx = np.array([idx_map[y] for y in y_true])\n",
    "    topk = proba_df.values.argsort(axis=1)[:, -k:]\n",
    "    hit = (topk == true_idx.reshape(-1,1)).any(axis=1)\n",
    "    return float(hit.mean())\n",
    "\n",
    "\n",
    "def eval_multiclass(y_true, y_pred, proba_df):\n",
    "    metrics = {}\n",
    "    metrics[\"accuracy\"] = accuracy_score(y_true, y_pred)\n",
    "    metrics[\"macro_f1\"] = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    # log_lossは確率が必要。クラスが1クラスしかないと失敗するので例外処理\n",
    "    try:\n",
    "        metrics[\"log_loss\"] = log_loss(y_true, proba_df.values, labels=list(proba_df.columns))\n",
    "    except Exception:\n",
    "        metrics[\"log_loss\"] = np.nan\n",
    "    metrics[\"top3_acc\"] = top_k_accuracy(y_true, proba_df, k=3)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_confmat(cm, classes, title, outpath):\n",
    "    plt.figure()\n",
    "    im = plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.colorbar(im)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 学習/評価 ルーチン\n",
    "# ---------------------------\n",
    "def train_eval_global(df, feature_cols, label, exp_name, out_dir):\n",
    "    \"\"\"hashvinを特徴に含めた共通モデル\"\"\"\n",
    "    use_cols = feature_cols + [label]\n",
    "    data = TabularDataset(df[use_cols])\n",
    "    predictor = TabularPredictor(\n",
    "        label=label, problem_type=\"multiclass\",\n",
    "        eval_metric=\"accuracy\",\n",
    "        path=os.path.join(out_dir, f\"predictor_{exp_name}_global\")\n",
    "    ).fit(\n",
    "        train_data=data,\n",
    "        presets=\"best_quality\"\n",
    "    )\n",
    "    # ホールドアウトがないので同データでの再評価 + 予測確率\n",
    "    y_true = data[label].astype(str).tolist()\n",
    "    y_pred = predictor.predict(data[feature_cols]).astype(str).tolist()\n",
    "    proba = predictor.predict_proba(data[feature_cols])\n",
    "\n",
    "    metrics = eval_multiclass(y_true, y_pred, proba)\n",
    "\n",
    "    # 混同行列\n",
    "    classes = sorted(proba.columns.tolist())\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    plot_confmat(cm, classes,\n",
    "                 f\"Confusion Matrix (GLOBAL, {exp_name})\",\n",
    "                 os.path.join(out_dir, f\"confmat_global_{exp_name}.png\"))\n",
    "\n",
    "    return predictor, metrics\n",
    "\n",
    "\n",
    "def train_eval_per_hashvin(df, feature_cols, label, exp_name, out_dir):\n",
    "    \"\"\"hashvinごとに独立モデル。結果を平均/加重平均でも報告。\"\"\"\n",
    "    results = []\n",
    "    per_models = {}\n",
    "    classes_global = sorted(df[label].astype(str).unique().tolist())\n",
    "    all_true, all_pred, all_proba_rows = [], [], []\n",
    "    # 一括確率配列を作るために列を合わせる\n",
    "    proba_cols_union = sorted(df[label].astype(str).unique())\n",
    "\n",
    "    for hv, sub in df.groupby(\"hashvin\"):\n",
    "        use_cols = feature_cols + [label]\n",
    "        data = TabularDataset(sub[use_cols])\n",
    "        if data.shape[0] < 10:\n",
    "            # データが少なすぎる場合はスキップ\n",
    "            continue\n",
    "\n",
    "        predictor = TabularPredictor(\n",
    "            label=label, problem_type=\"multiclass\",\n",
    "            eval_metric=\"accuracy\",\n",
    "            path=os.path.join(out_dir, f\"predictor_{exp_name}_hv_{hv}\")\n",
    "        ).fit(\n",
    "            train_data=data,\n",
    "            presets=\"medium_quality\"\n",
    "        )\n",
    "        y_true = data[label].astype(str).tolist()\n",
    "        y_pred = predictor.predict(data[feature_cols]).astype(str).tolist()\n",
    "        proba = predictor.predict_proba(data[feature_cols])\n",
    "\n",
    "        # 欠けているクラス列があれば0で補完（log_loss計算のため）\n",
    "        for c in proba_cols_union:\n",
    "            if c not in proba.columns:\n",
    "                proba[c] = 0.0\n",
    "        proba = proba[proba_cols_union]\n",
    "\n",
    "        metrics = eval_multiclass(y_true, y_pred, proba)\n",
    "        results.append({\"hashvin\": hv, **metrics, \"n\": len(y_true)})\n",
    "        per_models[hv] = predictor\n",
    "\n",
    "        # 連結用\n",
    "        all_true.extend(y_true)\n",
    "        all_pred.extend(y_pred)\n",
    "        all_proba_rows.append(proba.values)\n",
    "\n",
    "        # 混同行列（個別保存）\n",
    "        classes = sorted(proba.columns.tolist())\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "        plot_confmat(cm, classes,\n",
    "                     f\"Confusion Matrix (PER-HV {hv}, {exp_name})\",\n",
    "                     os.path.join(out_dir, f\"confmat_per_{exp_name}_{hv}.png\"))\n",
    "\n",
    "    # 集約（全サンプル結合での再評価）\n",
    "    if len(all_true) > 0:\n",
    "        all_proba = np.vstack(all_proba_rows)\n",
    "        # columnsはproba_cols_union\n",
    "        all_proba_df = pd.DataFrame(all_proba, columns=proba_cols_union)\n",
    "        agg_metrics = eval_multiclass(all_true, all_pred, all_proba_df)\n",
    "    else:\n",
    "        agg_metrics = {\"accuracy\": np.nan, \"macro_f1\": np.nan, \"log_loss\": np.nan, \"top3_acc\": np.nan}\n",
    "\n",
    "    per_df = pd.DataFrame(results)\n",
    "    if not per_df.empty:\n",
    "        # 件数加重平均\n",
    "        w = per_df[\"n\"].values\n",
    "        agg_weighted = {\n",
    "            \"accuracy\": np.average(per_df[\"accuracy\"], weights=w),\n",
    "            \"macro_f1\": np.average(per_df[\"macro_f1\"], weights=w),\n",
    "            \"log_loss\": np.average(per_df[\"log_loss\"].fillna(0), weights=w),  # log_lossのNaNは0扱い\n",
    "            \"top3_acc\": np.average(per_df[\"top3_acc\"], weights=w),\n",
    "        }\n",
    "    else:\n",
    "        agg_weighted = {\"accuracy\": np.nan, \"macro_f1\": np.nan, \"log_loss\": np.nan, \"top3_acc\": np.nan}\n",
    "\n",
    "    return per_models, per_df, agg_metrics, agg_weighted\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# サンプルデータ生成（CSV未指定時）\n",
    "# ---------------------------\n",
    "def synthesize_sample(n_hashvin=5, seed=42):\n",
    "    \"\"\"\n",
    "    PoCに近い生成規則:\n",
    "    - hashvin: 5台\n",
    "    - charge_cluster_id: 8種\n",
    "    - inactive_cluster_id: 6種（目的変数）\n",
    "    - charge_start_time: 90日分のランダム時刻\n",
    "    - ラベルは (hashvin, charge_cluster_id, hour帯, 曜日) に依存して確率的に決まる\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    hashvins = [f\"H{d:03d}\" for d in range(1, n_hashvin+1)]\n",
    "    charge_clusters = [f\"C{c:02d}\" for c in range(1, 9)]\n",
    "    inactive_clusters = [f\"P{p:02d}\" for p in range(1, 7)]\n",
    "\n",
    "    rows = []\n",
    "    t0 = datetime(2025, 6, 1, 0, 0, 0)\n",
    "    for hv in hashvins:\n",
    "        n = rng.integers(60, 220)  # 台ごとにデータ量を変える\n",
    "        for i in range(int(n)):\n",
    "            cc = rng.choice(charge_clusters, p=np.array([3,3,2,2,2,2,1,1])/16)\n",
    "            dt = t0 + timedelta(days=int(rng.integers(0, 90)),\n",
    "                                hours=int(rng.integers(0,24)),\n",
    "                                minutes=int(rng.integers(0,60)))\n",
    "            # 確率分布を生成（「それっぽい」規則）\n",
    "            hour = dt.hour\n",
    "            wday = dt.weekday()\n",
    "            base = rng.random(len(inactive_clusters))\n",
    "            # 充電場所×時間帯により特定ラベルを強める\n",
    "            bias_idx = (hash(cc) + hour) % len(inactive_clusters)\n",
    "            base[bias_idx] += 1.5\n",
    "            # 平日昼はP01/P02、週末夕方はP04を強める等\n",
    "            if wday < 5 and 11 <= hour <= 14:\n",
    "                base[0] += 1.0; base[1] += 0.8\n",
    "            if wday >= 5 and 17 <= hour <= 21:\n",
    "                base[3] += 1.2\n",
    "            # hashvin固有バイアス\n",
    "            base[hash(hv) % len(inactive_clusters)] += 1.0\n",
    "            prob = base / base.sum()\n",
    "            y = rng.choice(inactive_clusters, p=prob)\n",
    "\n",
    "            rows.append({\n",
    "                \"hashvin\": hv,\n",
    "                \"charge_cluster_id\": cc,\n",
    "                \"inactive_cluster_id\": y,\n",
    "                \"charge_start_time\": dt\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# メイン\n",
    "# ---------------------------\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--csv\", type=str, default=None, help=\"入力CSVへのパス（列名は案件準拠）\")\n",
    "    parser.add_argument(\"--out\", type=str, default=\"outputs\", help=\"結果出力ディレクトリ\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    os.makedirs(args.out, exist_ok=True)\n",
    "\n",
    "    # 1) データ読み込み or 生成\n",
    "    if args.csv is None:\n",
    "        print(\"[INFO] CSV未指定のためサンプルデータを生成します。\")\n",
    "        df = synthesize_sample(n_hashvin=5, seed=42)\n",
    "    else:\n",
    "        raw = pd.read_csv(args.csv)\n",
    "        # 必須列チェック\n",
    "        needed = [\"hashvin\", \"charge_cluster_id\", \"inactive_cluster_id\", \"charge_start_time\"]\n",
    "        missing = [c for c in needed if c not in raw.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"必要列が不足: {missing}\")\n",
    "        df = raw.copy()\n",
    "        # 文字列→datetime\n",
    "        df[\"charge_start_time\"] = pd.to_datetime(df[\"charge_start_time\"])\n",
    "\n",
    "    # 2) 型整形（ID類はカテゴリ/str、ラベルもstr）\n",
    "    df = ensure_categorical(df, [\"hashvin\", \"charge_cluster_id\", \"inactive_cluster_id\"])\n",
    "\n",
    "    # 3) 実験セットアップ: raw/engineered の2条件\n",
    "    experiments = [\n",
    "        {\"name\": \"time_raw\", \"mode\": \"raw\"},\n",
    "        {\"name\": \"time_engineered\", \"mode\": \"engineered\"},\n",
    "    ]\n",
    "\n",
    "    summary_rows = []\n",
    "    per_detail_frames = []\n",
    "\n",
    "    for exp in experiments:\n",
    "        exp_name = exp[\"name\"]\n",
    "        mode = exp[\"mode\"]\n",
    "        print(f\"\\n===== Experiment: {exp_name} ({mode}) =====\")\n",
    "\n",
    "        # 特徴生成\n",
    "        df_feat, time_feats = make_time_features(df, \"charge_start_time\", mode=mode)\n",
    "\n",
    "        # 共通: 必須の charge_cluster_id\n",
    "        base_feats = [\"hashvin\", \"charge_cluster_id\"]  # 共通モデルではhashvinも使う\n",
    "        # rawなら time_feats = [\"charge_start_time\"], engineeredなら派生一式\n",
    "        features_global = base_feats + time_feats\n",
    "\n",
    "        # --- 共通モデル（hashvinを含める）---\n",
    "        global_pred, global_metrics = train_eval_global(\n",
    "            df_feat, features_global, \"inactive_cluster_id\", exp_name, args.out\n",
    "        )\n",
    "        summary_rows.append({\n",
    "            \"setting\": f\"GLOBAL_{exp_name}\",\n",
    "            **global_metrics\n",
    "        })\n",
    "\n",
    "        # --- 個別モデル（hashvinごと）---\n",
    "        # 個別モデルでは特徴からhashvinは外す（固定値のため）\n",
    "        features_per = [\"charge_cluster_id\"] + time_feats\n",
    "        per_models, per_df, agg_metrics, agg_weighted = train_eval_per_hashvin(\n",
    "            df_feat, features_per, \"inactive_cluster_id\", exp_name, args.out\n",
    "        )\n",
    "        per_df[\"setting\"] = f\"PER_{exp_name}\"\n",
    "        per_detail_frames.append(per_df)\n",
    "\n",
    "        summary_rows.append({\n",
    "            \"setting\": f\"PER_{exp_name}_ALL-CONCAT\",  # 全件結合での再評価\n",
    "            **agg_metrics\n",
    "        })\n",
    "        summary_rows.append({\n",
    "            \"setting\": f\"PER_{exp_name}_WEIGHTED\",    # 件数加重平均\n",
    "            **agg_weighted\n",
    "        })\n",
    "\n",
    "    # 4) まとめ出力\n",
    "    summary = pd.DataFrame(summary_rows)\n",
    "    summary = summary[[\"setting\", \"accuracy\", \"top3_acc\", \"macro_f1\", \"log_loss\"]]\n",
    "    summary.sort_values(\"setting\", inplace=True)\n",
    "    summary_path = os.path.join(args.out, \"summary_metrics.csv\")\n",
    "    summary.to_csv(summary_path, index=False)\n",
    "    print(\"\\n=== SUMMARY (CSV saved) ===\")\n",
    "    print(summary)\n",
    "\n",
    "    if per_detail_frames:\n",
    "        per_detail = pd.concat(per_detail_frames, ignore_index=True)\n",
    "        per_detail = per_detail[[\"setting\", \"hashvin\", \"n\", \"accuracy\", \"top3_acc\", \"macro_f1\", \"log_loss\"]]\n",
    "        per_detail_path = os.path.join(args.out, \"per_hashvin_metrics.csv\")\n",
    "        per_detail.to_csv(per_detail_path, index=False)\n",
    "        print(\"\\n=== PER-HASHVIN METRICS (CSV saved) ===\")\n",
    "        print(per_detail.head())\n",
    "\n",
    "    print(f\"\\nOutputs saved in: {os.path.abspath(args.out)}\")\n",
    "    print(\" - summary_metrics.csv : グローバル/個別 × 時間特徴(生/加工)の比較\")\n",
    "    print(\" - per_hashvin_metrics.csv : 個別モデルの詳細（hashvin別）\")\n",
    "    print(\" - confmat_*.png : 混同行列（グローバル/個別）\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    # Jupyterからの不要な引数を除去\n",
    "    sys.argv = [sys.argv[0]]\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c62960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
