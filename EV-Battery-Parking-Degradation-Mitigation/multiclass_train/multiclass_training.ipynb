{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EV長時間放置クラスタ予測ノートブック\n",
        "要件.md（§0〜§8）に沿ってデータ前処理・学習・評価・成果物出力をhashvin単位で実行します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. セットアップ\n",
        "ここではPythonパッケージと社内モジュールを読み込み、要件§0〜§2で求められる『hashvin単位の独立処理』を行う準備をします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b3a1cfb3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# §0〜§2: パス設定とユーティリティ読込（hashvinごとの独立処理を守る前提）\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# ノートブックは multiclass_train ディレクトリで実行する想定（他hashvinの情報を混ぜない）\n",
        "PROJECT_DIR = Path.cwd()\n",
        "if str(PROJECT_DIR.parent) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_DIR.parent))\n",
        "\n",
        "from multiclass_train.pipeline import FeatureToggleConfig, PipelineConfig, HashvinProcessor\n",
        "from multiclass_train.trainer import train_and_evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc9b0b73",
      "metadata": {},
      "source": [
        "## 1. 設定\n",
        "要件§3〜§7で利用するハイパーパラメータ・閾値を定義します。ステーション種別は将来的に有効化できるようフラグで制御します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "22fd861d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# §3〜§7: パイプライン設定（HEAD抽出K、特徴量トグルなど）\n",
        "feature_toggles = FeatureToggleConfig(\n",
        "    use_distance=True,          # MVP: 距離\n",
        "    use_frequency=True,         # MVP: 頻度\n",
        "    use_recency=True,           # MVP: Recency\n",
        "    use_time_compat=True,       # MVP: time_compat\n",
        "    use_behavior_flags=True,    # ＋α: 行動帯フラグ（必要に応じてOFFに切替）\n",
        "    use_station_type=False,     # ＋α: ステーション種別（外部情報追加時にON推奨）\n",
        ")\n",
        "\n",
        "pipeline_config = PipelineConfig(\n",
        "    head_k=10,  # §3: HEADクラスタの上限 K≤10\n",
        "    feature_toggles=feature_toggles,\n",
        ")\n",
        "\n",
        "autogluon_presets = 'medium_quality'  # §6: AutoGluon Tabular のプリセット\n",
        "autogluon_time_limit = 300  # 学習時間の上限（秒）\n",
        "eval_thresholds = (0.5, 0.7, 0.9)  # §7: Top-1@τ の閾値候補"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. データ読込\n",
        "要件§1の入力仕様に沿ってCSVを読み込み、処理対象のhashvinを確認します。セルの出力にはhashvin一覧が表示されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hashvin count: 1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['hv_0001_demo']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# §1: 充電イベントと放置イベントの入力CSVを読み込む\n",
        "SESSIONS_PATH = PROJECT_DIR / 'ev_sessions_1.csv'\n",
        "CHARGE_PATH = PROJECT_DIR / 'ev_charge_to_long_inactive.csv'\n",
        "\n",
        "charge_df = pd.read_csv(CHARGE_PATH)\n",
        "sessions_df = pd.read_csv(SESSIONS_PATH)\n",
        "\n",
        "# hashvinごとの学習を徹底するため一覧を確認（出力: hashvinのリスト）\n",
        "hashvin_list = sorted(charge_df['hashvin'].unique())\n",
        "target_hashvins = hashvin_list  # 一部だけ試す場合はこのリストを編集\n",
        "print(f'hashvin count: {len(hashvin_list)}')\n",
        "hashvin_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 学習・評価実行\n",
        "要件§2〜§8をhashvin単位で順次実行し、\n",
        "esult/<hashvin>/ に評価レポートや予測詳細を保存します。セルの標準出力には進捗とサマリ指標が表示されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.4.0\n",
            "Python Version:     3.12.10\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.26200\n",
            "CPU Count:          16\n",
            "Memory Avail:       17.08 GB / 31.17 GB (54.8%)\n",
            "Disk Space Avail:   835.16 GB / 930.73 GB (89.7%)\n",
            "===================================================\n",
            "Presets specified: ['medium_quality']\n",
            "Using hyperparameters preset: hyperparameters='default'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Processing hv_0001_demo ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Beginning AutoGluon training ... Time limit = 300s\n",
            "AutoGluon will save models to \"c:\\workspace\\src\\kaggle\\ml-study\\EV-Battery-Parking-Degradation-Mitigation\\multiclass_train\\result\\hv_0001_demo\\autogluon\"\n",
            "Train Data Rows:    16\n",
            "Train Data Columns: 48\n",
            "Tuning Data Rows:    2\n",
            "Tuning Data Columns: 48\n",
            "Label Column:       y_class\n",
            "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
            "\t2 unique label values:  ['I_202', 'I_101']\n",
            "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
            "Problem Type:       binary\n",
            "Preprocessing data ...\n",
            "Warning: Updated label_count_threshold from 10 to 4 to avoid cutting too many classes.\n",
            "Selected class <--> label mapping:  class 1 = I_202, class 0 = I_101\n",
            "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (I_202) vs negative (I_101) class.\n",
            "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "Warning: dtype Int8 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
            "Cannot interpret 'Int8Dtype()' as a data type\n",
            "Warning: dtype Int8 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
            "Cannot interpret 'Int8Dtype()' as a data type\n",
            "Warning: dtype Int8 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
            "Cannot interpret 'Int8Dtype()' as a data type\n",
            "\tAvailable Memory:                    17511.31 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "Warning: dtype Int8 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
            "Cannot interpret 'Int8Dtype()' as a data type\n",
            "Warning: dtype Int8 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
            "Cannot interpret 'Int8Dtype()' as a data type\n",
            "Warning: dtype Int8 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
            "Cannot interpret 'Int8Dtype()' as a data type\n",
            "Warning: dtype Int8 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
            "Cannot interpret 'Int8Dtype()' as a data type\n",
            "Warning: dtype Int8 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
            "Cannot interpret 'Int8Dtype()' as a data type\n",
            "Warning: dtype Int8 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
            "Cannot interpret 'Int8Dtype()' as a data type\n",
            "Warning: dtype Int8 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
            "Cannot interpret 'Int8Dtype()' as a data type\n",
            "Warning: dtype Int8 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
            "Cannot interpret 'Int8Dtype()' as a data type\n",
            "Warning: dtype Int8 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
            "Cannot interpret 'Int8Dtype()' as a data type\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "Warning: dtype Int8 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
            "Cannot interpret 'Int8Dtype()' as a data type\n",
            "Warning: dtype Int8 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
            "Cannot interpret 'Int8Dtype()' as a data type\n",
            "Warning: dtype Int8 is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\n",
            "Cannot interpret 'Int8Dtype()' as a data type\n",
            "\t\t\tNote: Converting 13 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 11): ['time_compat_I_201', 'time_compat_I_203', 'daily_bin_06_12_cluster', 'freq_hashvin_I_100', 'recency_I_100_h', 'freq_hashvin_I_201', 'recency_I_201_h', 'freq_hashvin_I_102', 'recency_I_102_h', 'freq_hashvin_I_203', 'recency_I_203_h']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tUnused Original Features (Count: 10): ['is_return_band', 'is_commute_band', 'weekend_flag', 'dist_to_I_100', 'dist_to_I_201', 'transition_from_prev_to_I_201', 'dist_to_I_102', 'transition_from_prev_to_I_102', 'dist_to_I_203', 'transition_from_prev_to_I_203']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('Int8', [])  : 3 | ['is_return_band', 'is_commute_band', 'weekend_flag']\n",
            "\t\t('float', []) : 7 | ['dist_to_I_100', 'dist_to_I_201', 'transition_from_prev_to_I_201', 'dist_to_I_102', 'transition_from_prev_to_I_102', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 23 | ['hour_sin', 'hour_cos', 'time_compat_I_101', 'time_compat_I_202', 'time_compat_I_100', ...]\n",
            "\t\t('int', [])    :  1 | ['dow']\n",
            "\t\t('object', []) :  3 | ['daily_bin_00_06_cluster', 'daily_bin_12_18_cluster', 'daily_bin_18_24_cluster']\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  1 | ['daily_bin_00_06_cluster']\n",
            "\t\t('float', [])     : 19 | ['hour_sin', 'hour_cos', 'time_compat_I_101', 'time_compat_I_202', 'time_compat_I_100', ...]\n",
            "\t\t('int', [])       :  1 | ['dow']\n",
            "\t\t('int', ['bool']) :  6 | ['time_compat_I_102', 'daily_bin_12_18_cluster', 'daily_bin_18_24_cluster', 'daily_bin_06_12_minutes', 'dist_to_I_101', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t27 features in original data used to generate 27 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.00 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.07s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}],\n",
            "\t'XGB': [{}],\n",
            "\t'FASTAI': [{}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT ... Training model for up to 299.93s of the 299.93s of remaining time.\n",
            "\tFitting with cpus=8, gpus=0, mem=0.0/17.0 GB\n",
            "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
            "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
            "Fitting model: LightGBM ... Training model for up to 297.55s of the 297.55s of remaining time.\n",
            "\tFitting with cpus=8, gpus=0, mem=0.0/17.0 GB\n",
            "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
            "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
            "Fitting model: RandomForestGini ... Training model for up to 297.46s of the 297.46s of remaining time.\n",
            "\tFitting with cpus=16, gpus=0\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t0.3s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr ... Training model for up to 297.13s of the 297.13s of remaining time.\n",
            "\tFitting with cpus=16, gpus=0\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t0.29s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 296.82s of the 296.82s of remaining time.\n",
            "\tFitting with cpus=8, gpus=0\n",
            "\tWarning: Exception caused CatBoost to fail during training (ImportError)... Skipping this model.\n",
            "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==1.4.0`.\n",
            "Fitting model: ExtraTreesGini ... Training model for up to 296.73s of the 296.73s of remaining time.\n",
            "\tFitting with cpus=16, gpus=0\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t0.29s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr ... Training model for up to 296.42s of the 296.42s of remaining time.\n",
            "\tFitting with cpus=16, gpus=0\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t0.28s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ... Training model for up to 296.11s of the 296.10s of remaining time.\n",
            "\tFitting with cpus=8, gpus=0, mem=0.0/17.0 GB\n",
            "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
            "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.4.0`. \n",
            "Fitting model: XGBoost ... Training model for up to 296.02s of the 296.01s of remaining time.\n",
            "\tFitting with cpus=8, gpus=0\n",
            "\tWarning: Exception caused XGBoost to fail during training (ImportError)... Skipping this model.\n",
            "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.4.0`.\n",
            "Fitting model: NeuralNetTorch ... Training model for up to 295.92s of the 295.92s of remaining time.\n",
            "\tFitting with cpus=8, gpus=0, mem=0.0/17.0 GB\n",
            "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t0.64s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ... Training model for up to 295.27s of the 295.27s of remaining time.\n",
            "\tFitting with cpus=8, gpus=0, mem=0.0/16.9 GB\n",
            "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
            "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.93s of the 295.15s of remaining time.\n",
            "\tEnsemble Weights: {'RandomForestGini': 1.0}\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 4.88s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 130.2 rows/s (2 batch size)\n",
            "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2 rows).\n",
            "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\workspace\\src\\kaggle\\ml-study\\EV-Battery-Parking-Degradation-Mitigation\\multiclass_train\\result\\hv_0001_demo\\autogluon\")\n",
            "These features in provided data are not utilized by the predictor and will be ignored: ['time_compat_I_201', 'time_compat_I_203', 'daily_bin_06_12_cluster', 'is_return_band', 'is_commute_band', 'weekend_flag', 'dist_to_I_100', 'freq_hashvin_I_100', 'recency_I_100_h', 'dist_to_I_201', 'freq_hashvin_I_201', 'recency_I_201_h', 'transition_from_prev_to_I_201', 'dist_to_I_102', 'freq_hashvin_I_102', 'recency_I_102_h', 'transition_from_prev_to_I_102', 'dist_to_I_203', 'freq_hashvin_I_203', 'recency_I_203_h', 'transition_from_prev_to_I_203']\n",
            "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:534: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "c:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:534: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "\"['charge_end_time'] not in index\"",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m     result = processor.build_features()  \u001b[38;5;66;03m# §2〜§5: 特徴生成とHEAD抽出\u001b[39;00m\n\u001b[32m     14\u001b[39m     hashvin_dir = RESULT_ROOT / hashvin\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     metrics = \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhashvin_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable_station_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpipeline_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43menable_station_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mautogluon_presets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautogluon_presets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mautogluon_time_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautogluon_time_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_thresholds\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_thresholds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# §6〜§8: AutoGluon学習と評価レポート出力\u001b[39;00m\n\u001b[32m     23\u001b[39m     metrics_summary.append(metrics)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metrics_summary:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\EV-Battery-Parking-Degradation-Mitigation\\multiclass_train\\trainer.py:439\u001b[39m, in \u001b[36mtrain_and_evaluate\u001b[39m\u001b[34m(result, output_dir, enable_station_type, autogluon_presets, autogluon_time_limit, eval_thresholds)\u001b[39m\n\u001b[32m    435\u001b[39m         metrics[\u001b[33m\"\u001b[39m\u001b[33mexplanation_features\u001b[39m\u001b[33m\"\u001b[39m] = top_explain_features\n\u001b[32m    438\u001b[39m \u001b[38;5;66;03m# §7・§8: テストセットの予測詳細をCSVで保存し、分析できる状態にする\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m \u001b[43m_save_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproba_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplanation_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpredictions_test.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m (output_dir / \u001b[33m\"\u001b[39m\u001b[33mmetrics.json\u001b[39m\u001b[33m\"\u001b[39m).write_text(json.dumps(metrics, indent=\u001b[32m2\u001b[39m, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m), encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\EV-Battery-Parking-Degradation-Mitigation\\multiclass_train\\trainer.py:245\u001b[39m, in \u001b[36m_save_predictions\u001b[39m\u001b[34m(df, y_pred, proba_df, explanation_df, output_path)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"テストデータの予測結果と確率をCSV出力（§7, §8の可観測性向上）。\"\"\"\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[38;5;66;03m# 若手メモ: session_uidと充電時刻をセットで残すことで、運用時の「どのセッション？」追跡が容易になる。\u001b[39;00m\n\u001b[32m    244\u001b[39m \u001b[38;5;66;03m# 予測確率にimpact列を結合すると説明資料が作りやすくなるため、explanation_dfは後から横結合している。\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m export_df = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msession_uid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcharge_end_time\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43my_class\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m.copy()\n\u001b[32m    246\u001b[39m export_df.rename(columns={\u001b[33m\"\u001b[39m\u001b[33my_class\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33my_true\u001b[39m\u001b[33m\"\u001b[39m}, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    247\u001b[39m export_df[\u001b[33m\"\u001b[39m\u001b[33my_pred\u001b[39m\u001b[33m\"\u001b[39m] = y_pred\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4112\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4115\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\workspace\\src\\kaggle\\ml-study\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mKeyError\u001b[39m: \"['charge_end_time'] not in index\""
          ]
        }
      ],
      "source": [
        "# §2〜§8: hashvinごとの前処理・学習・評価・成果物保存フロー\n",
        "RESULT_ROOT = PROJECT_DIR / 'result'\n",
        "RESULT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "metrics_summary = []\n",
        "for hashvin in target_hashvins:\n",
        "    print(f'=== Processing {hashvin} ===')  # 進捗ログ\n",
        "    charge_subset = charge_df[charge_df['hashvin'] == hashvin].copy()\n",
        "    sessions_subset = sessions_df[sessions_df['hashvin'] == hashvin].copy()\n",
        "\n",
        "    processor = HashvinProcessor(hashvin, charge_subset, sessions_subset, pipeline_config)\n",
        "    result = processor.build_features()  # §2〜§5: 特徴生成とHEAD抽出\n",
        "\n",
        "    hashvin_dir = RESULT_ROOT / hashvin\n",
        "    metrics = train_and_evaluate(\n",
        "        result,\n",
        "        hashvin_dir,\n",
        "        enable_station_type=pipeline_config.enable_station_type,\n",
        "        autogluon_presets=autogluon_presets,\n",
        "        autogluon_time_limit=autogluon_time_limit,\n",
        "        eval_thresholds=eval_thresholds,\n",
        "    )  # §6〜§8: AutoGluon学習と評価レポート出力\n",
        "    metrics_summary.append(metrics)\n",
        "\n",
        "if metrics_summary:\n",
        "    summary_path_json = RESULT_ROOT / 'metrics_summary.json'\n",
        "    summary_path_csv = RESULT_ROOT / 'metrics_summary.csv'\n",
        "    summary_path_json.write_text(json.dumps(metrics_summary, indent=2, ensure_ascii=False), encoding='utf-8')\n",
        "    pd.DataFrame(metrics_summary).to_csv(summary_path_csv, index=False)\n",
        "    display(pd.DataFrame(metrics_summary))  # Jupyter上で指標を可視化\n",
        "else:\n",
        "    print('No hashvin processed. Check target_hashvins list.')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
