# EV 最適 SOC 誘導リコメンドにおける再トレーニング戦略整理  
（更新頻度・学習期間・トリガ設計と参考ソース）

---

## 1. 目的

本検討の目的は、  
**「EV 最適 SOC 誘導型リコメンド・ソリューション」における機械学習モデルの再トレーニング戦略（周期・トリガ・学習データ期間・評価方法）を整理し、PoC 以降の運用イメージと推奨方針を明確化すること**  
です。

本ソリューションはユーザーの充電行動に介入する性質を持つため、  
**導入前後でデータ分布が変化することを前提とした再トレーニング設計**が必要となります。

---

## 2. 背景

### 2.1 ソリューション概要

本ソリューションは、ユーザーごとの充電習慣を学習し、

- 「今回の充電をスキップしても、次回・次々回の通常充電タイミングまで SOC が持つか」
- 「高 SOC 長時間放置を減らしつつ、走行距離不足リスクを抑えられるか」

を予測したうえで、**充電タイミングのリコメンド**を行うものです。

これにより、

- 不要な高 SOC 長時間放置や過剰充電を減らし、バッテリ劣化抑制に寄与
- 一方で、ユーザー利便性（走行距離不足・不安）の悪化を最小限に抑制

することを狙います。

### 2.2 ソリューション導入によるデータ変化の可能性

リコメンドによりユーザー行動が変化し、次のような変化が想定されます。

- リコメンドに従い、**従来とは異なるタイミングで充電する**ユーザーの増加
- 充電スキップの増加に伴う、**SOC 利用レンジの変化（低 SOC 側の利用増加など）**
- 高 SOC 長時間放置ケースの減少

したがって、学習データは

- 導入前：リコメンドなしの「素の行動データ」
- 導入後：リコメンドに影響を受けた「行動変容後のデータ」

という、性質の異なる 2 種類が混在することになります。

このため、

- **再トレーニングの頻度・トリガ**
- **学習に用いるデータ期間（どこまで過去を使うか）**

をセットで設計することが、本ソリューションの重要論点となります。

---

## 3. 今回明らかにする目標

本検討では、以下の 3 点を明らかにします。

1. **再トレーニングの基本方針**
   - どのような種類のトリガ（定期・性能ドリフト・データドリフト・ビジネスイベント）を組み合わせるべきか
   - 共通モデルを複数ユーザーで利用する場合のモニタリング単位（全体 / セグメント / 個人）の考え方

2. **PoC〜初期運用における「推奨ステップ」**
   - どの程度の周期で再トレーニングを行うのが現実的か（コスト vs 精度）
   - どのような指標をモニタリングし、どのような条件で追加学習をトリガすべきか

3. **学習に用いるデータ期間の設計**
   - 初回学習・再学習時に、どれくらい過去のデータを使うのが妥当か
   - リコメンド導入前データと導入後データをどう扱い分けるか（どこかで「切り替える」のか、重み付けするのか）

---

## 4. 検討内容

### 4.1 再トレーニング方針の分類（広く候補整理）

再トレーニングのトリガ候補は、大きく以下 4 種類に分類できます。

1. **スケジュールベース（定期実行）**
   - 日次・週次・月次・四半期などで定期的に再学習
   - シンプルかつ予測可能な運用が可能

2. **モデル性能ドリフトに基づくトリガ**
   - 直近のオンラインデータに対する予測精度（MAE, RMSE など）やビジネス指標（不足率、リコメンド採用率）が劣化した際に再学習を検討

3. **データドリフトに基づくトリガ**
   - 特徴量分布が学習時と比べて変化した場合に再学習を検討
   - 統計的指標（PSI, KS 検定など）や異常値率の変化を利用

4. **ビジネス / プロダクトイベントに基づくトリガ**
   - 新車種追加、充電料金体系変更、新 UI/機能追加など、ユーザー行動が一気に変わるイベント発生時に再学習を実施

一般的な MLOps のベストプラクティスでも、**時間ベースの再学習とドリフト検知に基づく再学習を組み合わせるハイブリッド構成**が推奨されています。

---

### 4.2 定期実行（スケジュールベース）の考え方

#### 4.2.1 周期決定の判断軸

周期を決める際の主な観点は以下の 3 点です。

1. **データの変化スピード**
2. **学習コスト（計算資源・運用工数）**
3. **精度劣化への許容度（ユーザー体験・安全性への影響）**

#### 4.2.2 EV SOC リコメンド文脈での候補

- PoC・初期運用段階
  - **月次再学習**をベースとし、データの蓄積状況・変動を見ながら週次への細分化を検討
- 商用段階
  - **月次をベース**としつつ、
  - 性能ドリフトやデータドリフトが顕著な場合に「追加再学習」を行うハイブリッド構成

（一般的な文献でも、「モデルドリフトが急激でない場合、月次・四半期などの比較的粗い定期再学習＋ドリフト検知によるトリガ」というパターンが推奨されています。）

---

### 4.3 モデル性能ドリフトに基づくトリガ

#### 4.3.1 導入する指標候補

- 回帰指標
  - 直近 n 日の MAE / RMSE / MAPE
- ビジネス指標
  - リコメンド採用率（提案どおりに充電スキップ/実施された割合）
  - SOC 不足発生率（閾値 SOC を下回るケースの割合）
  - 高 SOC 長時間放置率

#### 4.3.2 共通モデル × 複数ユーザーの場合のモニタリング粒度

- **全体指標**
  - 全ユーザーの誤差・不足率をまとめた指標
- **セグメント別指標（推奨）**
  - 地域・気温帯・車種・利用スタイル（短距離中心/長距離中心）などのセグメント毎に集計

多くの MLOps ベストプラクティスでも、**全体指標に加えてセグメント別の性能モニタリングを行い、特定セグメントの悪化を検知する**ことが推奨されています。

#### 4.3.3 トリガ設定例

- 全体 MAE がベースライン比 **+15% 以上悪化 × 7 日連続**
- 主要セグメント MAE がベースライン比 **+20% 以上悪化 × 3 日連続**
- SOC 不足発生率が目標値（例：1%）を **一定期間継続して超過**

これらの条件を満たした場合に、

- 再学習ジョブの実行
- セグメント別モデルの導入検討
- 一時的なリコメンド強度の調整（より安全側に寄せる）

などを行います。

---

### 4.4 データドリフトに基づくトリガ

#### 4.4.1 観点

リコメンドによりユーザー行動が変わると、入力特徴量の分布も変化します。

- 高 SOC 領域データの減少
- 充電開始時間帯分布の変化
- 走行距離分布の変化　など

これらを **「意図した行動変容」なのか、「想定外の環境変化・センサー異常」なのか** を見極めつつ監視する必要があります。

#### 4.4.2 検知方法の例

- 特徴量分布の比較
  - Population Stability Index (PSI)
  - KS 検定などの統計的テスト
- 重要特徴量へのフォーカス
  - SHAP 等で重要な特徴量に対し、分布変化を重点監視
- 異常値比率の変化
  - Isolation Forest 等での異常スコアの分布変化

PSI は特に金融・信用スコアリング領域で**データドリフト検知の定番指標**として広く使われており、最近の MLOps 文脈でも標準的なドリフト指標として紹介されています。  
また、Evidently AI や Datadog などのドキュメントでも、**入力データの分布ドリフトを性能ドリフトの前兆指標として監視する**ことが推奨されています。

#### 4.4.3 トリガ設定の例

- 重要特徴量のうち、PSI が「中〜高ドリフト」閾値を超えた割合が一定以上
- 異常値比率がトレーニング時の 2 倍以上に増加

ただし、**リコメンド導入後の一定期間は「意図されたデータ分布変化」でもある**ため、

- 「ドリフトを検知したらすぐ再学習」ではなく、
- 「ドリフトの状況をモニタしつつ、一定期間分のポスト導入データが溜まったタイミングで再学習を計画的に実施」

という運用が現実的です。

---

### 4.5 ビジネス / プロダクトイベントに基づくトリガ

EV 文脈で想定されるイベント例：

- 新車種・バッテリタイプの追加
- 充電料金体系の変更
- アプリ UI の大幅刷新（リコメンドの見せ方変更）

これらのイベントはユーザー行動やデータ分布に大きな影響を及ぼす可能性があり、  
**性能ドリフトを待たずに再トレーニングやモデル見直しを事前計画する**ことが一般的な推奨となっています。

---

### 4.6 本プロジェクト向け「再トレーニング推奨ステップ」

#### Step 1. PoC 期間中の方針

1. **再トレーニング周期**
   - 基本：**月次再学習**をベースとする
   - PoC 中はモデル更新による影響も把握したいので、過度に頻度を上げず、評価・検証の余裕を持つ

2. **モニタリング指標の実装**
   - オンライン指標
     - 各ユーザー / セグメントのリコメンド採用率
     - SOC 不足発生率
     - 高 SOC 長時間放置率
   - モデル性能指標（可能な範囲で）
     - 直近 n 日の MAE / RMSE（教師データが取得できるラベルに対して）

3. **データドリフトの簡易監視**
   - SOC 分布
   - 充電開始時間帯
   - 放置時間
   など、EV 特有の主要特徴量について、導入前後の分布比較を実施

4. **モデル更新の実験的評価**
   - 「月次再学習あり / なし」などのパターンをオフラインでシミュレーションし、
   - 精度差とコストを評価 → 本運用フェーズの再学習頻度設計のインプットとする

#### Step 2. 初期本番運用での推奨構成

1. **ベースラインとしての月次再学習**
   - 月 1 回、直近数ヶ月〜 12 ヶ月のデータを用いて再学習

2. **性能ドリフトモニタリングと追加トリガ**
   - 全体＋主要セグメント別の MAE / 不足率の監視
   - 閾値を超えた場合には「追加再学習」を検討
   - 障害や一時的ノイズとの切り分けは運用ルールで定義（例：一定日数連続での悪化を条件にする）

3. **リコメンド導入後の「新世界」データへの徐々のシフト**
   - 導入初期は「導入前データ + 導入後データ」で学習
   - 時間経過とともに「導入前データのウェイトを下げる / 切り捨てる」方針も検討
   - 長期的には「導入後行動」を前提としたモデルに収束させる

4. **ビジネスイベント時の再学習計画**
   - 新車種・料金体系変更などのイベント発生時には、
     - 事前に「影響分析 + 再学習スケジュール」を策定

---

### 4.7 学習データ期間（どれくらい過去を学習に使うか）

#### 4.7.1 基本的な考え方（トレードオフ）

どれくらい過去のデータを学習に使うかには、以下のトレードオフがあります。

- **過去を長く取る（長い期間を使う）**
  - メリット
    - 学習データ量が増え、統計的に安定したモデルになりやすい
    - 季節性・年周期など長期パターンを拾いやすい
  - デメリット
    - 古い行動パターン（リコメンド導入前など）を引きずり、**現状のユーザー行動とズレるリスク**
    - 環境変化・料金変更などの前の世界を引きずる

- **過去を短く取る（直近データ中心）**
  - メリット
    - 現状に即したモデルになりやすい（最新の行動変容を反映しやすい）
  - デメリット
    - データ量が不足し、ノイズに敏感・不安定になりやすい
    - 長期的な季節性が捉えにくくなる（夏と冬の差など）

一般的なタイムシリーズやドリフト対応の文献では、**スライディングウィンドウ（直近 N 期間のみを使う）や拡張ウィンドウ（徐々にデータを追加）といった枠組みで、このトレードオフを扱う**ことが推奨されています。

概念ドリフトが大きい場合は、「古いデータを丸ごと捨てて、新しい環境に合わせて再学習するべき」とする実務向け記事もあります。

#### 4.7.2 代表的な設計パターン

1. **全履歴学習（フルヒストリーモデル）**
   - 常に利用可能な全期間のデータを学習に使用
   - 長期的な傾向と季節性を最大限活用できる
   - ただし、古い「世界」のデータを引きずるため、強い概念ドリフトには弱い

2. **スライディングウィンドウ（ローリング）**
   - 例：**直近 6〜12 ヶ月**だけを学習に利用し、それより古いデータは切り捨て
   - ドリフトに対応しやすく、最新行動にフィットしやすい
   - ウィンドウサイズを季節性（1年周期など）とバランスさせるのがポイント

3. **拡張ウィンドウ（expanding window）**
   - 最初は短い期間から始め、再学習のたびに**過去データを追加していく**
   - データが少ない立ち上がりフェーズには有効
   - 「いつ古いデータを切り捨てるか」は別途設計が必要

4. **時間減衰（time-decay）重み付け**
   - すべての期間のデータを使いつつ、「新しいデータほど重みを高くする」
   - 長期情報と最近の動向の両方をバランス良く利用可能

5. **二段構え：ベースモデル＋最新データでの微調整**
   - 大きな履歴データで「共通ベースモデル」を学習
   - 直近数ヶ月のデータだけを用いてファインチューニング・キャリブレーション
   - 概念ドリフトを吸収しつつ、少ないデータで柔軟に追随可能

これらのパターンは、金融時系列や需要予測など、**時間とともに分布が変わる領域で広く使われているアプローチ**です。

#### 4.7.3 本ソリューション向けの具体的な推奨

EV 充電・SOC 文脈と、今回の PoC〜初期運用フェーズを前提に、次のような設計が妥当と考えます。

1. **初回モデル（PoC / 導入直前）の学習期間**
   - 可能であれば、**少なくとも直近 12 ヶ月分**のデータを利用
     - 理由
       - 夏・冬の気温差や暖房・冷房使用など、**季節性を 1 周分はカバー**したい
       - 車両利用パターンも年単位で周期がある（長期休暇、年末年始など）
   - データが潤沢な場合（車両台数が多いなど）は、直近 24 ヶ月まで広げることも検討  
     （ただし古い料金体系・仕様変更が多い場合は注意）

2. **導入後、再学習時の「学習ウィンドウ」**
   - 基本案：
     - **直近 12 ヶ月のスライディングウィンドウ**を学習に利用
   - 理由：
     - 季節性を 1 周分含みつつ、古すぎる行動パターン（リコメンド導入前など）を徐々に排除
     - データ量とモデル安定性のバランスが取りやすい

3. **リコメンド導入前データの扱い**
   - 導入直後〜数ヶ月間は、**導入前データも含めた「長めの拡張ウィンドウ」**で学習し、
     - データ量の不足を補う
   - 導入後 6〜12 ヶ月経過し、「リコメンドありの世界」のデータが十分に蓄積した段階で、
     - **ウィンドウから導入前データを段階的に外していく**（もしくは重みを下げる）
   - 強い制度変更や大規模 UI 変更などで行動パターンが大きく変わった場合には、
     - 当該イベント前のデータを「別世界」とみなし、**以降のデータだけで再学習**することも選択肢

4. **セグメント別・ユーザー別のデータ期間**
   - 個人レベルのデータは十分に貯まるまで時間がかかるため、
     - **共通ベースモデルは 12 ヶ月ウィンドウ**
     - **個人別補正（パーソナライズ）は直近数ヶ月（例：3〜6 ヶ月）**
   - データの少ないユーザー・セグメントでは
     - 強めの正則化や hierarchical モデルを使い、「共通モデルから大きく逸脱しない」ように設計

---

## 5. 結論

- 再トレーニング戦略は、
  - **ベースラインとしての定期実行（本案件では月次を想定）**
  - **性能ドリフト・データドリフト・ビジネスイベントに基づく追加トリガ**
  を組み合わせるハイブリッド構成が妥当と考えられます。
- 学習データ期間については、
  - 初回モデルは **直近 12 ヶ月以上（可能であれば 24 ヶ月）** を利用し、季節性を含めた共通モデルを構築
  - 再学習時は **直近 12 ヶ月のスライディングウィンドウ** を基本とし、
    - 強い概念ドリフトがある場合は古いデータを除外
    - 導入前データは、導入後データが十分に溜まり次第、重みを下げる or ウィンドウ外へ
- リコメンド導入によりユーザー行動そのものが変わるため、
  - **「導入前の世界」と「導入後の世界」を意識してデータ期間を設計すること**
  - **導入後データが一定量たまった段階で「導入後世界に最適化したモデル」にシフトすること**
  が、長期的な性能維持の観点から重要です。

---

## 6. 課題

1. **ラベリング遅延と評価タイムラグ**
   - SOC 消費量・不足発生など、ラベル確定に時間がかかるため、性能ドリフトの検知も遅延する

2. **行動変容下での評価設計の難しさ**
   - リコメンドに従った場合と従わなかった場合の差分評価（因果的評価・A/B テスト）が必要

3. **セグメント・ユーザーごとのデータ量の偏り**
   - データが少ないセグメントでは、安定した指標やパーソナライズが難しい

4. **MLOps 基盤の整備**
   - 再学習・評価・デプロイをパイプライン化しないと、現実的な頻度での運用が困難

---

## 7. 次のアクション

1. **PoC 向け指標・閾値の具体化**
   - 不足率・採用率・高 SOC 放置率など、オンライン指標とトリガ条件を定義

2. **再学習頻度と学習期間のシミュレーション**
   - 「月次・週次・再学習なし」「12 ヶ月 vs 6 ヶ月ウィンドウ」など、オフラインで比較

3. **簡易 MLOps パイプラインの設計**
   - データ取得 → 前処理 → 学習 → 評価 → モデル登録 → デプロイを一連のフローとして整理

4. **導入前後の比較分析設計**
   - 導入前後で充電行動・SOC 分布がどう変わるかを比較可能なログ・分析設計を行う

---

## 8. 参考文献・ソースとその位置づけ

※代表的なもののみ記載。  
本資料の具体的な提案は、以下のベストプラクティスを EV SOC リコメンドの文脈に落とし込んだものです。

### 8.1 モデルドリフト・再トレーニング全般

1. **AI Multiple: “Why & How to Retrain ML Models?”**  
   - URL  
     - https://research.aimultiple.com/model-retraining/  
   - 概要  
     - モデル再学習が必要になる理由（データドリフト、概念ドリフト、ビジネス環境の変化など）と、再学習戦略（時間ベース・トリガベース）の種類を整理した記事。
     - 特に「古いデータが現環境を反映しなくなった場合はデータセットを入れ替えるべき」という実務的な観点が示されている。
   - 本資料での引用箇所  
     - 4.1「再トレーニング方針の分類」の考え方
     - 4.7.1「強い概念ドリフト時には古いデータを切り捨てる」という方針の根拠

2. **Neptune.ai: “Retraining Model During Deployment: Continuous Training & Testing”**  
   - URL  
     - https://neptune.ai/blog/retraining-model-during-deployment-continuous-training-continuous-testing  
   - 概要  
     - 本番環境における継続的な再学習・テストの重要性と、「時間ベースの再学習」と「ドリフト検知に基づく再学習」を組み合わせるアーキテクチャを解説。
   - 本資料での引用箇所  
     - 4.1「定期実行＋トリガのハイブリッド構成が一般的」という整理
     - 4.2「月次・四半期などの粗い定期再学習＋ドリフト検知」の考え方

3. **IBM: “What Is Model Drift?”**  
   - URL  
     - https://www.ibm.com/think/topics/model-drift  
   - 概要  
     - モデルドリフトの定義（入力分布や入力–出力関係の変化に起因する性能劣化）と、継続的モニタリング・再学習の必要性を解説。
   - 本資料での引用箇所  
     - 4.3「モデル性能ドリフト」の説明
     - 5「結論」内の「継続的な再学習が必要」という前提

4. **Encord: “Model Drift: Best Practices to Improve ML Model Performance”**  
   - URL  
     - https://encord.com/blog/model-drift-best-practices/  
   - 概要  
     - データドリフト・ラベルドリフト・概念ドリフトの整理、および「自動モニタリング＋再学習トリガ」のベストプラクティスを解説。
   - 本資料での引用箇所  
     - 4.3.2「セグメント別モニタリングが必要」という考え方
     - 5「ハイブリッド構成が妥当」という判断の補強

---

### 8.2 モデル監視・トリガベース再学習

1. **Datadog: “Machine learning model monitoring: Best practices”**  
   - URL  
     - https://www.datadoghq.com/blog/ml-model-monitoring-in-production-best-practices/  
   - 概要  
     - 本番環境の ML モデル監視において、性能指標だけでなく、入力データ・予測分布のドリフトを監視することの重要性を解説。
   - 本資料での引用箇所  
     - 4.3「性能ドリフト指標の設計」
     - 4.4「データドリフトを性能劣化の前兆として監視する」考え方

2. **Microsoft Tech Community: “Identifying drift in ML models”**  
   - URL  
     - https://techcommunity.microsoft.com/t5/fasttrack-for-azure/identifying-drift-in-ml-models/ba-p/4040531  
   - 概要  
     - Azure 環境におけるドリフト検知と、ドリフトをトリガとした再学習のフローを紹介。
   - 本資料での引用箇所  
     - 4.1「ドリフト検知に基づく再学習」の位置付け
     - 4.5「ビジネスイベント時の再学習計画」の考え方の補強

3. **Medium: “Mastering Model Retraining in MLOps”**  
   - URL  
     - https://randomtrees.medium.com/mastering-model-retraining-in-mlops-4bb961ee7070  
   - 概要  
     - 時間ベース再学習とトリガベース再学習（性能指標・統計指標）を組み合わせる実務的な再学習戦略を解説。
   - 本資料での引用箇所  
     - 4.1「再トレーニング方針の分類」
     - 5「ハイブリッド構成が妥当」という結論部分

---

### 8.3 データドリフト検知と PSI

1. **Fiddler: “Measuring Data Drift with the Population Stability Index (PSI)”**  
   - URL  
     - https://www.fiddler.ai/blog/measuring-data-drift-population-stability-index  
   - 概要  
     - PSI の定義、計算方法、ドリフトレベルの解釈（低・中・高ドリフト）を解説。
   - 本資料での引用箇所  
     - 4.4.2「データドリフト検知方法の例（PSI）」
     - 4.4.3「PSI を用いたトリガ設定」のイメージ

2. **Coralogix: “A Practical Introduction to Population Stability Index (PSI)”**  
   - URL  
     - https://coralogix.com/ai-blog/a-practical-introduction-to-population-stability-index-psi/  
   - 概要  
     - PSI の実務的な使い方と、どの程度の値をドリフトとみなすかのガイドラインを紹介。
   - 本資料での引用箇所  
     - 4.4.3「PSI による閾値設定」のイメージ

3. **Acceldata: “Detecting and Managing Data Drift”**  
   - URL  
     - https://www.acceldata.io/blog/data-drift  
   - 概要  
     - データドリフトの種類（スキュー、シフトなど）と、検知→対処（再学習・フラグ立てなど）の一般的フローを説明。
   - 本資料での引用箇所  
     - 4.4「データドリフトを性能劣化の前兆として扱う」考え方全般

4. **Arize: “PSI: What You Need To Know”**  
   - URL  
     - https://arize.com/blog-course/population-stability-index-psi/  
   - 概要  
     - モデル監視の文脈で PSI をどう使うか、どのようなケースで有効かを解説。
   - 本資料での引用箇所  
     - 4.4.2「PSI を MLOps のドリフト指標として使う」部分

5. **DataCamp: “Understanding Data Drift and Model Drift”**  
   - URL  
     - https://www.datacamp.com/tutorial/understanding-data-drift-model-drift  
   - 概要  
     - データドリフト・モデルドリフトの概要と、監視指標の例を整理したチュートリアル。
   - 本資料での引用箇所  
     - 4.4「データドリフトとモデルドリフトの基本整理」

6. **Evidently AI: “What is data drift in ML, and how to detect and handle it”**  
   - URL  
     - https://www.evidentlyai.com/ml-in-production/data-drift  
   - 概要  
     - ドリフト検知ツールとしての Evidently を例に、実務でのドリフト監視・可視化の手法を解説。
   - 本資料での引用箇所  
     - 4.4.2「PSI/KS を用いた分布比較」
     - 4.4.3「ドリフトスコアをトリガとして扱う」部分

---

### 8.4 学習データ期間・スライディングウィンドウ

1. **Lazy Programmer: “What is the Sliding Window Method in Time Series Analysis?”**  
   - URL  
     - https://lazyprogrammer.me/what-is-the-sliding-window-method-in-time-series-analysis/  
   - 概要  
     - スライディングウィンドウによる時系列予測の基本を解説。直近 N 期間のデータを用いてモデルを更新していく考え方を紹介。
   - 本資料での引用箇所  
     - 4.7.2「スライディングウィンドウ（ローリング）」の説明
     - 4.7.3「直近 12 ヶ月ウィンドウを基本とする」という設計方針の根拠

2. **Medium: “Using Windowing for Features and Labels in Time-Series Forecasting”**  
   - URL  
     - https://medium.com/@ailuminastudio/using-windowing-for-features-and-labels-in-time-series-forecasting-b4b653cbe776  
   - 概要  
     - 時系列予測でのウィンドウ設計（特徴量・ラベルの生成）を説明。ウィンドウサイズがモデル性能やドリフト耐性に与える影響を議論。
   - 本資料での引用箇所  
     - 4.7.1「学習期間のトレードオフ」の整理
     - 4.7.2「ウィンドウサイズと季節性のバランス」の説明

3. **各種論文・レビュー（例：需要予測・金融時系列のスライディングウィンドウ手法）**  
   - 例：  
     - https://link.springer.com/article/10.1007/s10462-024-10851-x  
     - https://www.ripublication.com/ijcir17/ijcirv13n5_46.pdf  
     - https://milvus.io/ai-quick-reference/how-do-time-series-models-handle-concept-drift  
   - 概要  
     - スライディングウィンドウや時間減衰重み付け、アンサンブルによるドリフト対応が一般的に用いられていることを示す事例。
   - 本資料での引用箇所  
     - 4.7.2「設計パターン一覧」
     - 4.7.3「初回は長めの履歴＋以降は 12 ヶ月ウィンドウ」という案の妥当性の裏付け

---

この資料全体としては、  
上記の一般的な MLOps・ドリフト・時系列のベストプラクティスを踏まえつつ、

- 「EV バッテリ劣化抑制」という目的
- 「充電タイミングリコメンドにより行動変容が起きる」という前提

に合わせて、再トレーニング頻度・トリガ・学習データ期間を設計した、という位置づけになります。
