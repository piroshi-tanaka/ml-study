# SageMaker MLOps 定量コスト試算レポート（概算 / us-east-1 固定）

- 対象リージョン: **us-east-1 (N. Virginia)**
- 対象ユーザー数: **100,000**
- 提供地域: 日本 / 北米 / 欧州（ただし **コスト試算は us-east-1 単一リージョン**で固定）
- 目的: モデル構成 **A/B/C** の差分が出る領域（再学習・監視・モデル管理）を中心に、**定量的な月額コスト**を提示する

---

## 0. スコープ整理（今回の算出対象 / 対象外）

### 0-1. 算出対象（本レポート）
- (3) 再学習トリガー
  - スケジュール
  - データドリフト / モデルドリフト（監視ジョブ）
- (4) 再学習パイプライン（前処理 → 学習 → 評価/ゲート → デプロイ）
  - SageMaker Processing / Training の **ジョブ実行コスト**
  - 評価ゲート（評価ジョブ）も **Processing と同等に課金が発生**
- (5) モデル管理コスト
  - モデル成果物（S3 保管）・バージョン保持
  - Model Registry は主にメタデータ管理（実質は成果物の S3 コストが支配的）
- (6) 推論コスト
  - リアルタイム推論 / サーバレス / バッチ（Batch Transform）

### 0-2. 対象外（ユーザー要請により除外）
- (1) データ基盤（S3 + ETL/クエリ + 監視ログ）: **既存基盤を利用する前提で除外**
- (2) CI/CD（モデル/前処理のビルド・配布）: **除外**
- ECR / Feature Store 等の付随コスト: **除外**（※ただし将来の商用化時は別途見積推奨）

### 0-3. 注意（本試算で除外または簡略化しているが、商用では効きやすい項目）
- **インターネット向けデータ転送（Data Transfer Out）**  
  単一リージョンで日米欧へ配信する場合、レスポンス転送量次第で費用が増える可能性あり（今回は未算入）
- **可用性/冗長化要件**（Multi-AZ、Multi-Region、DR、WAF 等）：未算入
- **アプリ側の DB/キュー/通知基盤**：未算入（推論方式により必要）

---

## 1. モデル構成パターン定義（A/B/C）

| パターン | 定義 | モデル数 |
|---|---|---:|
| A | 完全個人ごとに 1 モデル | 100,000 |
| B | ユーザーグループごとに 1 モデル | **G 個**（本試算では例として **G=200**） |
| C | 共通モデル | 1 |

> B の G はビジネス/データ上のセグメント数で変動するため、G をパラメータとして式でも提示します。

---

## 2. ワークロード前提（推論イベント回数）

ユーザー要件に基づき、月あたり推論回数を以下で仮定します。

- 「その日の初乗車イベント」: 1日1回 → **30 回/月**
- 「充電前タイミング」: 3日に1回 → **10 回/月**
- 合計: **40 回/ユーザー/月**

よって、月間リクエスト数:
- **100,000 × 40 = 4,000,000 リクエスト/月**

---

## 3. 課金が発生する AWS サービスと「何に課金されるか」

### 3-1. 再学習・評価（SageMaker Processing / Training）
| サービス | 使い所 | 課金単位（支配要因） | コストが増える要因 |
|---|---|---|---|
| SageMaker Processing | 前処理、特徴量生成、評価、ドリフト集計（自作監視含む） | **インスタンス時間（インスタンス数 × 実行時間）** | ① 実行時間増、② 大きいインスタンス、③ 並列ジョブ数増（＝同時に走るインスタンス数増） |
| SageMaker Training | 学習（GBDT/NN 等） | **インスタンス時間（インスタンス数 × 実行時間）** | ① 実行時間増、② 大きい/高単価インスタンス、③ 分散学習でインスタンス数増 |

**重要: 「1人あたりの時間×人数」で良いのか？**  
- **総コスト（$）は、基本的に「合計インスタンス時間」に比例**します。  
  つまり「1ジョブあたり実行時間 × ジョブ数」で総時間を見積もるのは合理的です。  
- ただし **並列化は“壁時計時間（完了までの時間）”を短縮するための手段**であり、  
  並列化しても **同じ総インスタンス時間ならコストは大差ありません**。  
- 一方で、並列化のために「より大きいインスタンスへ変更」「分散で台数増」すると、**総インスタンス時間が増え**、コスト増につながります。

### 3-2. 評価ゲート（Evaluation Gate）
- ゲート判定は「評価処理（Processing）」として走るため、**Processing の実行コストが発生**します。
- “自動デプロイだから人件費ゼロ”は概ね正しい一方、**計算資源コストは残る**点に注意。

### 3-3. ドリフト監視（Model Monitor or 自作監視）
| 方式 | 何ができる | 課金 | 制約 |
|---|---|---|---|
| SageMaker Model Monitor | データ品質/モデル品質/ドリフトを定期実行 | **監視ジョブのインスタンス時間 +（データ処理量課金が発生するケースあり）** | 監視設計（サンプリング率、頻度）でコスト可変 |
| 自作監視（Processing で集計） | 任意指標を実装可能 | **Processing 実行コスト** | 実装/運用の手間は増える |

**注意（重要）**  
- **SageMaker Serverless Inference は Model Monitor 等の一部機能が使えない**ため、  
  Serverless を選ぶ場合は「自作監視」に寄せる必要が出ます（＝設計差分が発生）。  
  ※Serverless の “Feature Exclusions” に明記されています。

### 3-4. モデル管理（Model Registry / S3）
- Model Registry 自体はメタデータ管理が中心で、一般に支配的なのは **モデル成果物を保存する S3 の容量コスト**です。
- A のようにモデル数が 100k になると、**“モデル数 × バージョン保持数 × モデルサイズ”**が効いてきます。

### 3-5. 再学習トリガー（EventBridge）
- 定期実行は EventBridge Scheduler 等で実装するのが一般的。  
- 課金は「スケジュール実行回数（invocations）」に比例。大量でも **$1 / 100万実行**程度で、通常は小さい部類。

---

## 4. インスタンスタイプの選び方（m5 / c5 / r5 / サイズの違い）

### 4-1. まず押さえる命名規則（例: ml.m5.xlarge）
- **ml.<ファミリー><世代>.<サイズ>** という構造
  - m: 汎用（バランス型）
  - c: 計算最適化（CPU性能寄り、メモリ少なめ）
  - r: メモリ最適化（大容量メモリ）
  - サイズ（xlarge, 2xlarge, 4xlarge...）は vCPU/メモリが増える

### 4-2. 代表例（SageMaker での CPU インスタンス仕様）
以下は代表スペック例（vCPU/メモリ）。  
- ml.m5.xlarge: 4 vCPU / 16 GiB  
- ml.c5.xlarge: 4 vCPU / 8 GiB  
- ml.r5.xlarge: 4 vCPU / 32 GiB  

**違いの直感**
- **m5**: 前処理・学習・軽量推論の“困ったらまず”の選択肢
- **c5**: 推論など CPU バウンド（計算寄り）で、メモリ要件が小さい場合に効く
- **r5**: 特徴量が巨大、モデルが巨大、またはデータをメモリに載せて高速化したい場合に効く

### 4-3. 推奨の選定手順（実務的）
1. **メモリ要件**を満たす最小サイズを選ぶ（OOM を避ける）
2. 次に **CPU 使用率と実行時間**を見る（CPU が詰まるなら c 系へ）
3. **ジョブ実行ログ**（Processing/Training）と推論メトリクスから、段階的にサイズ調整
4. 推論は負荷試験で「1インスタンスあたりの rps と p95/p99 レイテンシ」を測り、必要台数を決める

---

## 5. 定量試算の前提（今回の“標準シナリオ”）

> 不足情報（実行時間、モデルサイズ、要求レイテンシ、入力サイズなど）は、PoC 段階で一般的な値を**明示して仮定**します。  
> 実際は計測で更新してください（式も併記するため、差し替え容易です）。

### 5-1. 再学習パイプライン（1モデルあたり）
| ステップ | サービス | インスタンス | 実行時間（仮定） |
|---|---|---|---:|
| 前処理 | Processing | ml.m5.xlarge | 15 分 |
| 学習 | Training | ml.m5.xlarge | 30 分 |
| 評価・ゲート | Processing | ml.m5.xlarge | 10 分 |
| 合計 |  |  | **55 分（=0.9167 時間）** |

- 単価（例）: ml.m5.xlarge = **$0.23 / 時間**（オンデマンド）  
- 1モデルあたり再学習1回コスト（概算）:  
  - 0.9167h × $0.23/h = **$0.2108 / モデル / 回**

### 5-2. 再学習頻度（標準シナリオ）
- 定期再学習: **1 回 / 月**
- ドリフト等での追加再学習: **+20%**（= 1.2 倍）  
  - → 合計 **1.2 回 / 月 相当**

> 週次/日次は「感度分析」で別途提示します。

### 5-3. モデル成果物（S3）保持（標準シナリオ）
- 1モデル成果物サイズ: **20 MB**（前処理含む軽量モデル想定）
- 保持バージョン数: **6**（半年保持のイメージ）
- S3 標準ストレージ単価（参考）: **$0.023/GB-month**（最初の 50TB 帯の代表値として使用）

### 5-4. 推論（標準シナリオ）
推論方式の比較用に、代表的な2パターンを出します。

#### (a) Serverless Inference（オンデマンド）
- メモリ: 2GB
- 平均推論時間: 100ms（=0.1秒）
- 1リクエストあたり入出力: **22KB**（入力20KB + 出力2KB）
- 単価（例）:
  - 推論時間: **$0.00004 / 秒**（2GB の例）
  - データ処理: **$0.016 / GB**

> 重要: Serverless は Model Monitor 等が使えないため、監視は “自作監視” で代替する想定。

#### (b) Real-time Endpoint（常時起動）
- 例として **ml.c5.xlarge × 2台**（最低限の冗長化想定）
- 単価（例）: ml.c5.xlarge = **$0.204 / 時間**
- データ処理: **$0.016 / GB**（例の単価に合わせる）

---

## 6. 主要コストの計算式（顧客が差し替え可能な形）

### 6-1. 再学習パイプライン（月額）
- 1モデルあたりコスト:  
  **C_train_per_model = (T_proc + T_train + T_eval) × P_instance**
- 月額（モデル数 M、月あたり再学習回数 R）:  
  **C_train_month = M × R × C_train_per_model**

※並列数を増やしても、**総インスタンス時間が同じならコストは同じ**  
（並列数は主に“完了までの時間”を短縮するパラメータ）

### 6-2. モデル成果物（S3）月額
- **C_model_storage = (M × ModelSizeGB × Versions) × P_S3**

### 6-3. 推論（月額）
#### Serverless（オンデマンド）
- **C_srvls = (Req × AvgSec × P_sec) + (DataGB × P_data)**

#### Real-time（常時起動）
- **C_rt = (N_instance × 730h × P_inst_hour) + (DataGB × P_data)**

### 6-4. ドリフト監視（月額）
- Model Monitor が使える場合:  
  **C_monitor = (N_jobs × JobHours × P_inst_hour) + (SampledDataGB × P_data)**  
- Serverless の場合（自作監視）:  
  **C_custom_monitor ≒ Processing の実行コスト**（上式に同様）

---

## 7. 試算結果（標準シナリオ / 月額 / us-east-1）

### 7-1. 入力値（固定）
- 月間推論リクエスト数: **4,000,000**
- 月間入出力データ量（22KB/req）: 約 **83.92 GiB**
- ドリフト監視: 1日1回・5分、サンプリング 1%（自作監視 or Model Monitor 相当）
  - 監視ジョブ時間: 2.5h/月
  - サンプルデータ: 0.839 GiB/月

### 7-2. パターン別（A/B/C）の“差が出る”領域
- 再学習パイプライン: A が圧倒的に重い
- モデル成果物保管: A で効きやすい（ただしモデルサイズが小さければ致命的ではない）
- ドリフト監視: **エンドポイント数が同じなら差は小さい**（A/B/C ではなく “設計” で差が出る）

### 7-3. 月額コスト（標準シナリオ：Bは G=200）

#### (A) Serverless 推論（※Model Monitor 非対応 → 自作監視）
| パターン | 再学習（R=1.2/月） | モデル保管（S3） | 推論（Serverless） | 監視（自作/相当） | 合計（概算） |
|---|---:|---:|---:|---:|---:|
| A (100k) | $25,300.00 | $269.53 | $17.34 | $0.59 | **$25,587.46 /月** |
| B (G=200) | $50.60 | $0.54 | $17.34 | $0.59 | **$69.07 /月** |
| C (1) | $0.25 | $0.00 | $17.34 | $0.59 | **$18.19 /月** |

#### (B) Real-time 推論（常時起動 + Model Monitor 可能）
| パターン | 再学習（R=1.2/月） | モデル保管（S3） | 推論（RT: c5.xlarge×2） | 監視（Model Monitor相当） | 合計（概算） |
|---|---:|---:|---:|---:|---:|
| A (100k) | $25,300.00 | $269.53 | $299.18 | $0.59 | **$25,869.30 /月** |
| B (G=200) | $50.60 | $0.54 | $299.18 | $0.59 | **$350.91 /月** |
| C (1) | $0.25 | $0.00 | $299.18 | $0.59 | **$300.03 /月** |

> 結論（標準シナリオ）  
> - **A は再学習が支配的**で、推論方式の違いは相対的に小さい。  
> - **B と C は推論コストが支配的**になりやすい（特に Real-time は常時起動のため）。  
> - **Serverless は非常に安いが、Model Monitor 等の機能制約がある**ため、監視設計が別途必要。

---

## 8. 「A/B/Cで差が出るのはどこか？」への回答（設計込み）

### 8-1. 差が確実に出る
- **再学習パイプライン（Processing/Training/Eval の実行回数）**
  - A: 100k モデルを回すほど線形に増える
  - B: G に比例
  - C: ほぼ固定
- **モデルレジストリ/成果物管理（S3 容量と PUT 回数）**
  - A: モデル数と保持バージョン数が増えるほど増加
- **ドリフト監視の“設計”**
  - 監視対象が「エンドポイント単位」か「モデル単位」かで差が出る  
  - 典型的には **エンドポイント単位で監視**するため、A/B/Cより「エンドポイント数」で決まるケースが多い

### 8-2. 推論コストは “同じ” になるか？
**結論: 条件次第で同じにもなるし、変わる。**
- 「A/B/C いずれも 1 エンドポイントで処理する」設計にできるなら、推論基盤コストはほぼ同じ
- ただし **A の“完全個人モデル”をそのままサーバ側で推論**しようとすると、
  - 100k モデルをどうロード/キャッシュするか（Multi-Model Endpoint 等）で **レイテンシやモデルロード I/O が増えやすい**
  - “ユーザーごとに別エンドポイント”は現実的ではなく、推論コストが破綻する
- よって A は、推論よりも **再学習と運用のスケール**がボトルネックになりやすい

---

## 9. 再学習ジョブ数・並列数の決め方（実務基準）

### 9-1. コスト観点の原則
- コストは **総インスタンス時間**に比例  
- 並列数は「どれだけ早く再学習を終わらせたいか（SLA）」で決める

### 9-2. 推奨の決定フロー
1. **1ジョブの実測時間**を取る（PoCで必須）
2. 月間ジョブ数（= モデル数×再学習回数）を見積る
3. 許容する“完了時間”から必要な並列数を逆算  
   - 例: Bで G=200、月1回、1ジョブ55分  
     - 並列 10 なら、だいたい 200/10×55分 ≒ 18.3時間で完了
4. AWS の同時実行上限（Service Quotas）を確認し、必要なら緩和申請

---

## 10. 感度分析（再学習頻度を週次にした場合）

- 月あたり週数換算: 約 4.33  
- ドリフト上乗せ 20% を同様に適用すると、**約 5.2 倍**になります。

| パターン | 月次（標準）再学習費 | 週次（概算）再学習費 |
|---|---:|---:|
| A (100k) | $25,300/月 | **$109,633/月** |
| B (G=200) | $50.6/月 | **$219/月** |
| C (1) | $0.253/月 | **$1.10/月** |

> A は頻度を上げるほど急激に増えるため、  
> “完全個人モデル”を維持する場合は、再学習頻度と対象ユーザーを絞る（アクティブ上位のみ等）設計が現実的です。

---

## 11. 推奨アーキテクチャ（コスト・監視・実装のバランス）

### 11-1. PoC 推奨（最小コスト・最小実装）
- **C（共通モデル）** から開始  
- 推論: **Serverless** または **Batch（夜間）**  
- 監視: Serverless の場合は **Processing で自作監視**（シンプルな統計監視）

### 11-2. 商用の現実解（パーソナライズとコストの折衷）
- B（グループモデル）または **共通モデル + パーソナライズ補正（軽量パラメータ）**
- 推論: 監視要件が強いなら **Real-time** を採用し Model Monitor を使う
- A（完全個人）をそのままやる場合は、再学習と運用が支配的なので
  - “学習対象をアクティブ上位に限定”
  - “更新頻度を下げる”
  - “個人差分は軽量パラメータのみ”
  などの制約設計が必須

---

## 12. 参照リンク（公式）と参照箇所

> リンクはすべて AWS 公式です。どの単価・制約・仕様に使ったかを明記します。

1) SageMaker AI Pricing（トレーニング/推論/Model Monitor/Serverless Inference の単価例）
- https://aws.amazon.com/sagemaker/ai/pricing/
- 参照箇所:
  - ml.m5.xlarge / ml.c5.xlarge の例単価
  - Serverless Inference の「$0.00004/秒」「$0.016/GB」の例
  - Real-time inference の例
  - Model Monitor の例

2) Serverless Inference の機能制約（Model Monitor 等が利用不可である点）
- https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html
- 参照箇所:
  - Feature Exclusions（Serverless で使えない機能の一覧）

3) SageMaker で利用できるインスタンスタイプ（vCPU/メモリ/用途分類）
- https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/notebooks-available-instance-types.html
- 参照箇所:
  - ml.m5 / ml.c5 / ml.r5 の vCPU・メモリ

4) EC2 インスタンス命名規則（ファミリー/世代/サイズ）
- https://docs.aws.amazon.com/ec2/latest/instancetypes/instance-type-names.html
- 参照箇所:
  - 命名規則の説明（series/generation/size）

5) Amazon EventBridge Pricing（スケジュール実行の単価根拠）
- https://aws.amazon.com/eventbridge/pricing/
- 参照箇所:
  - Scheduler invocations の $/million の根拠

6) S3 ストレージ単価（モデル成果物の S3 保管の単価根拠として）
- https://docs.aws.amazon.com/solutions/latest/live-streaming-on-aws-with-media-services/estimate-the-aws-cost-of-the-solution.html
- 参照箇所:
  - S3 Standard の $0.023/GB-month（代表値として記載されている箇所）

---

## 付録A: 差し替え用パラメータ一覧（ここだけ埋めれば再計算できる）
- ユーザー数 U = 100,000
- 1ユーザーあたり推論回数/月 = 40
- Req = U × 40
- 入出力サイズ/req = 22KB
- 再学習頻度 R（月あたり）
- モデル数:
  - A: M=U
  - B: M=G
  - C: M=1
- 再学習ジョブ時間:
  - T_proc, T_train, T_eval（時間）
- インスタンス単価:
  - P_m5_xlarge, P_c5_xlarge など
- モデルサイズ:
  - ModelSizeMB
- 保持バージョン:
  - Versions

