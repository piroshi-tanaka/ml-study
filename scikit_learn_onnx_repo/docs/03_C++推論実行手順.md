# 03. C++推論実行手順

## 📋 目次

1. [概要](#概要)
2. [前提条件](#前提条件)
3. [プロジェクトファイルの確認](#プロジェクトファイルの確認)
4. [ビルドの実行](#ビルドの実行)
5. [推論の実行](#推論の実行)
6. [結果の検証](#結果の検証)
7. [トラブルシューティング](#トラブルシューティング)
8. [応用・カスタマイズ](#応用カスタマイズ)

---

## 概要

### 目的

- C++でONNXモデルから推論を実行
- Python推論との精度を比較
- AUTOSAR環境での実現可能性を検証

### 期待される成果

1. **推論の成功**
   - テストデータ27サンプルすべてで推論成功

2. **精度の一致**
   - Python ONNX vs C++ ONNXの差分が 1e-5 以下
   - RMSE、MAE、R²がほぼ同じ

3. **環境の検証**
   - Linux環境でのC++推論が可能
   - AUTOSAR環境への展開可能性を確認

### 所要時間

- ビルド: 約1-2分
- 推論実行: 約数秒
- 合計: 約5-10分

---

## 前提条件

### 完了している必要があるステップ

✅ [01. PythonからONNX形式を作成する手順](./01_Python_ONNX作成手順.md)
- ONNXモデル、テストデータが作成済み

✅ [02. WSL Ubuntu環境構築手順](./02_WSL_Ubuntu_環境構築手順.md)
- WSL Ubuntu、ONNX Runtime C++がインストール済み

### 必要なファイル

以下のファイルが `cpp_inference/` ディレクトリに存在すること：

```
cpp_inference/
├── CMakeLists.txt            ✓ ビルド設定
├── onnx_inference.cpp        ✓ C++推論コード
├── time_series_model.onnx    ✓ ONNXモデル
├── test_data.csv             ✓ テストデータ
├── test_labels.csv           ✓ 正解ラベル
├── setup_and_build.sh        ✓ セットアップスクリプト
└── run_inference.sh          ✓ 実行スクリプト
```

---

## プロジェクトファイルの確認

### ステップ1: WSL Ubuntuを起動

**方法A: PowerShellから**
```powershell
wsl
```

**方法B: VS Codeから**
- Ubuntuターミナルで `code .` を実行
- VS Codeの統合ターミナルを開く（`Ctrl + `` ）

### ステップ2: プロジェクトディレクトリに移動

```bash
cd /mnt/c/workspace/src/ml-study/scikit_learn_onnx_repo/cpp_inference
```

**現在地を確認**:
```bash
pwd
```

**期待される出力**:
```
/mnt/c/workspace/src/ml-study/scikit_learn_onnx_repo/cpp_inference
```

### ステップ3: ファイルの存在確認

```bash
ls -la
```

**期待される出力**:
```
total XXX
drwxr-xr-x 1 horih horih    512 Nov  2 16:00 .
drwxr-xr-x 1 horih horih    512 Nov  2 16:00 ..
-rw-r--r-- 1 horih horih   XXXX Nov  2 16:00 CMakeLists.txt
-rw-r--r-- 1 horih horih   XXXX Nov  2 16:00 onnx_inference.cpp
-rw-r--r-- 1 horih horih XXXXXX Nov  2 16:00 time_series_model.onnx
-rw-r--r-- 1 horih horih   XXXX Nov  2 16:00 test_data.csv
-rw-r--r-- 1 horih horih   XXXX Nov  2 16:00 test_labels.csv
-rwxr-xr-x 1 horih horih   XXXX Nov  2 16:00 setup_and_build.sh
-rwxr-xr-x 1 horih horih   XXXX Nov  2 16:00 run_inference.sh
...
```

✅ これらのファイルが存在すればOK

❌ **もしファイルがない場合**:
- [01. PythonからONNX形式を作成する手順](./01_Python_ONNX作成手順.md) に戻る
- Jupyter Notebookのセル22を実行してデータをエクスポート

---

## ビルドの実行

### ステップ1: スクリプトに実行権限を付与

```bash
chmod +x setup_and_build.sh run_inference.sh
```

**確認**:
```bash
ls -l *.sh
```

**期待される出力**:
```
-rwxr-xr-x 1 horih horih XXXX Nov  2 16:00 run_inference.sh
-rwxr-xr-x 1 horih horih XXXX Nov  2 16:00 setup_and_build.sh
```

`rwx` の `x` が実行権限を表します

### ステップ2: ビルドディレクトリの準備

```bash
# 既存のbuildディレクトリがあれば削除（クリーンアップ）
rm -rf build

# 新しいbuildディレクトリを作成
mkdir build
cd build
```

**現在地を確認**:
```bash
pwd
```

**期待される出力**:
```
/mnt/c/workspace/src/ml-study/scikit_learn_onnx_repo/cpp_inference/build
```

### ステップ3: CMakeの実行

```bash
cmake ..
```

**期待される出力**:
```
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- ONNX Runtime Include Dir: /usr/local/include
-- ONNX Runtime Lib Dir: /usr/local/lib
-- Configuring done
-- Generating done
-- Build files have been written to: /mnt/c/.../cpp_inference/build
```

✅ **成功の確認**:
- 「**Configuring done**」が表示される
- 「**Generating done**」が表示される
- エラーメッセージが出ない

❌ **もしエラーが出たら**: [トラブルシューティング](#トラブルシューティング)へ

### ステップ4: makeの実行（ビルド）

```bash
make
```

**期待される出力**:
```
Scanning dependencies of target onnx_inference
[ 50%] Building CXX object CMakeFiles/onnx_inference.dir/onnx_inference.cpp.o
[100%] Linking CXX executable onnx_inference
[100%] Built target onnx_inference
```

✅ **成功の確認**:
- 「**Built target onnx_inference**」が表示される
- エラーメッセージが出ない

**所要時間**: 10秒〜1分

### ステップ5: ビルド成果物の確認

```bash
ls -l onnx_inference
```

**期待される出力**:
```
-rwxr-xr-x 1 horih horih XXXXXX Nov  2 16:45 onnx_inference
```

✅ 実行ファイルが作成されればビルド成功！🎉

---

## 推論の実行

### ステップ1: cpp_inferenceディレクトリに戻る

```bash
cd ..
pwd
```

**期待される出力**:
```
/mnt/c/workspace/src/ml-study/scikit_learn_onnx_repo/cpp_inference
```

### ステップ2: 実行スクリプトで推論を実行

```bash
./run_inference.sh
```

または直接実行：

```bash
./build/onnx_inference
```

### ステップ3: 実行結果の確認

**期待される出力の全体像**:

```
========================================
  ONNX推論実行
========================================

========================================
  ONNX時系列予測推論（C++版）
========================================

📂 ファイル読み込み中...
  ONNXモデル: time_series_model.onnx
  テストデータ: test_data.csv
  正解ラベル: test_labels.csv

✓ データ読み込み完了
  サンプル数: 27
  特徴量数: 12

🔧 ONNX Runtimeの初期化中...
  入力ノード数: 1
  出力ノード数: 1
  入力名: float_input
  出力名: variable

🚀 推論実行中...
  進捗: 10/27 サンプル
  進捗: 20/27 サンプル
  進捗: 27/27 サンプル

✓ 推論完了

============================================================
【精度評価】
============================================================

【C++ ONNX推論】
  RMSE: 42.123456
  MAE:  35.789012
  R²:   0.678901

【Python ONNX vs C++ ONNX】
  最大差分: 1.234567e-05
  平均差分: 3.456789e-06
  ✓ Python ONNXとC++ ONNXの予測はほぼ一致！

============================================================
【予測結果サンプル（最初の5件）】
============================================================
No.  実績値    Python    C++予測   誤差
------------------------------------------------------------
  1   504.00   315.42   315.42    188.58
  2   404.00   313.51   313.51     90.49
  3   359.00   310.35   310.35     48.65
  4   310.00   307.91   307.91      2.09
  5   337.00   308.76   308.76     28.24

============================================================
✓ すべての処理が完了しました！
============================================================

✓ 実行完了
```

**所要時間**: 数秒

---

## 結果の検証

### ✅ 成功の判定基準

#### 1. 推論の完了

- [ ] 「✓ データ読み込み完了」が表示された
- [ ] サンプル数が27と表示された
- [ ] 特徴量数が12と表示された
- [ ] 「✓ 推論完了」が表示された

#### 2. 精度評価

**【C++ ONNX推論】の数値を確認**:
```
RMSE: XX.XXXXXX  ← 値が表示されている
MAE:  XX.XXXXXX  ← 値が表示されている
R²:   X.XXXXXX   ← 0以上の値が表示されている
```

#### 3. Python推論との一致性 ⭐ 最重要

**【Python ONNX vs C++ ONNX】の差分を確認**:
```
最大差分: X.XXXXXXe-XX  ← 1e-05 (0.00001) 以下であること
平均差分: X.XXXXXXe-XX  ← 1e-06 (0.000001) 程度であること
```

✅ **「✓ Python ONNXとC++ ONNXの予測はほぼ一致！」**
→ この表示が出れば成功！

#### 4. 予測結果サンプル

**表の確認**:
- Python予測とC++予測の列がほぼ同じ値
- 小数点以下の差が非常に小さい

### 📊 結果の意味

#### 精度指標の解釈

| 指標 | 意味 | 良い値 |
|-----|------|-------|
| **RMSE** | 予測誤差の平均（単位：千人） | 小さいほど良い |
| **MAE** | 絶対誤差の平均（単位：千人） | 小さいほど良い |
| **R²** | 決定係数（0-1の範囲） | 1に近いほど良い |

#### Python vs C++の差分の意味

| 差分 | 評価 | 意味 |
|-----|------|------|
| < 1e-06 | ✅ 完璧 | 数値計算誤差レベル |
| < 1e-05 | ✅ 非常に良い | 実用上問題なし |
| < 1e-04 | ⚠️ 許容範囲 | 要確認 |
| > 1e-04 | ❌ 問題あり | 原因調査が必要 |

### 🎯 AUTOSAR環境への示唆

**今回の結果が示すこと**:

1. **精度の保証**
   - Python推論とC++推論がほぼ一致
   - AUTOSAR環境でも同等の精度が期待できる

2. **実現可能性**
   - Linux環境でC++推論が成功
   - ONNX Runtime C++ APIが正常に動作

3. **次のステップ**
   - メモリ使用量の測定
   - 推論時間の測定
   - クロスコンパイル環境の構築

---

## トラブルシューティング

### エラー1: CMakeでヘッダーファイルが見つからない

**エラー例**:
```
fatal error: onnxruntime_cxx_api.h: No such file or directory
```

**原因**: ONNX Runtimeのヘッダーファイルがインストールされていない

**解決策**:
```bash
# ヘッダーファイルの確認
ls /usr/local/include/onnxruntime/

# なければ再インストール
cd ~
sudo cp -r onnxruntime-linux-x64-1.18.0/include/*.h /usr/local/include/onnxruntime/
```

### エラー2: makeでライブラリが見つからない

**エラー例**:
```
cannot find -lonnxruntime
```

**原因**: ONNX Runtimeのライブラリがインストールされていない

**解決策**:
```bash
# ライブラリの確認
ls /usr/local/lib/libonnxruntime*

# なければ再インストール
cd ~
sudo cp -r onnxruntime-linux-x64-1.18.0/lib/* /usr/local/lib/
sudo ldconfig
```

### エラー3: 実行時に「cannot open shared object file」

**エラー例**:
```
error while loading shared libraries: libonnxruntime.so: cannot open shared object file
```

**原因**: ライブラリパスが通っていない

**解決策**:
```bash
# ライブラリキャッシュを更新
sudo ldconfig

# または環境変数を設定
export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH

# 再実行
./build/onnx_inference
```

### エラー4: ファイルが見つからない

**エラー例**:
```
❌ ファイルを開けません: time_series_model.onnx
```

**原因**: 必要なファイルがない

**解決策**:
```bash
# ファイルの存在確認
ls -la time_series_model.onnx
ls -la test_data.csv
ls -la test_labels.csv

# なければPython側で再エクスポート（Jupyter Notebookのセル22）
```

### エラー5: Permission denied

**エラー例**:
```
bash: ./run_inference.sh: Permission denied
```

**原因**: 実行権限がない

**解決策**:
```bash
chmod +x run_inference.sh
./run_inference.sh
```

### エラー6: 差分が大きい

**現象**: 「Python ONNXとC++ ONNXの予測はほぼ一致！」が表示されない

**原因**: データの不整合またはバージョン不一致

**解決策**:
1. Python側でノートブックを最初から再実行
2. データを再エクスポート（セル22）
3. C++側で再ビルド

```bash
cd /mnt/c/workspace/src/ml-study/scikit_learn_onnx_repo/cpp_inference
rm -rf build
mkdir build
cd build
cmake ..
make
cd ..
./run_inference.sh
```

---

## 応用・カスタマイズ

### 推論時間の測定

C++コードに以下を追加（`onnx_inference.cpp`）:

```cpp
#include <chrono>

// 推論前
auto start = std::chrono::high_resolution_clock::now();

// 推論実行（既存のコード）
auto output_tensors = session.Run(...);

// 推論後
auto end = std::chrono::high_resolution_clock::now();
auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);
std::cout << "推論時間: " << duration.count() / 1000.0 << " ms" << std::endl;
```

### バッチ推論の実装

複数サンプルを一度に推論する場合:

```cpp
// 入力形状を変更
std::vector<int64_t> input_shape = {batch_size, num_features};
```

### モデルの最適化

ONNX Runtimeの最適化オプション:

```cpp
Ort::SessionOptions session_options;
session_options.SetGraphOptimizationLevel(
    GraphOptimizationLevel::ORT_ENABLE_ALL
);
```

### メモリプロファイリング

```bash
# valgrindでメモリ使用量を測定
sudo apt install valgrind
valgrind --tool=massif ./build/onnx_inference
```

---

## ✅ 最終チェックリスト

すべての手順が完了したら、以下を確認してください：

### ビルド

- [ ] CMakeがエラーなく完了した
- [ ] makeがエラーなく完了した
- [ ] `build/onnx_inference` 実行ファイルが作成された

### 推論実行

- [ ] 「✓ データ読み込み完了」が表示された
- [ ] 「✓ 推論完了」が表示された
- [ ] すべてのサンプル（27個）で推論が成功した

### 精度検証

- [ ] RMSE、MAE、R²の値が表示された
- [ ] Python vs C++の差分が 1e-05 以下
- [ ] 「✓ Python ONNXとC++ ONNXの予測はほぼ一致！」が表示された

### 結果の理解

- [ ] 精度指標の意味を理解した
- [ ] AUTOSAR環境への示唆を理解した
- [ ] 次のステップを把握した

---

## 🎉 完了！

### 達成したこと

1. ✅ **Pythonでモデルをトレーニング**
   - scikit-learnで時系列予測モデルを作成
   - ONNX形式に変換

2. ✅ **WSL Ubuntu環境の構築**
   - Linux環境でC++開発環境を整備
   - ONNX Runtime C++ APIをインストール

3. ✅ **C++で推論を実行**
   - テストデータで推論を実行
   - Python推論との精度を比較

4. ✅ **AUTOSAR環境への実現可能性を検証**
   - Linux環境でC++推論が成功
   - 精度の一致を確認

### 次のステップ（オプション）

#### ステップA: パフォーマンス最適化

- 推論時間の測定と最適化
- メモリ使用量の削減
- バッチ推論の実装

#### ステップB: AUTOSAR環境への展開

1. **クロスコンパイル環境の構築**
   - AUTOSARターゲット用のツールチェーン
   - ONNX Runtimeのクロスコンパイル

2. **リアルタイム性の検証**
   - 推論時間の測定
   - 最悪実行時間（WCET）の評価

3. **安全性の考慮**
   - エラーハンドリングの強化
   - フェイルセーフ機構の実装

#### ステップC: 他のモデルへの応用

- 分類問題（Classification）
- 異常検知（Anomaly Detection）
- その他の回帰問題

---

## 📚 参考リンク

### ONNX Runtime

- [公式ドキュメント](https://onnxruntime.ai/docs/)
- [C++ API リファレンス](https://onnxruntime.ai/docs/api/c/)
- [GitHub リポジトリ](https://github.com/microsoft/onnxruntime)

### AUTOSAR

- [AUTOSAR公式サイト](https://www.autosar.org/)
- [AUTOSAR Adaptive Platform](https://www.autosar.org/standards/adaptive-platform/)

### 学習リソース

- [ONNX公式チュートリアル](https://onnx.ai/get-started.html)
- [scikit-learn → ONNX変換](https://onnx.ai/sklearn-onnx/)

---

## 📊 プロジェクト全体のまとめ

### ファイル構成

```
ml-study/
├── pyproject.toml
└── scikit_learn_onnx_repo/
    ├── time_series_onnx_demo.ipynb      # Pythonでの処理
    ├── time_series_model.onnx           # ONNXモデル
    ├── time_series_model.pkl            # scikit-learnモデル
    ├── *.png (4つ)                      # 可視化結果
    ├── docs/                            # ドキュメント
    │   ├── 01_Python_ONNX作成手順.md
    │   ├── 02_WSL_Ubuntu_環境構築手順.md
    │   └── 03_C++推論実行手順.md       # このファイル
    └── cpp_inference/                   # C++推論
        ├── CMakeLists.txt
        ├── onnx_inference.cpp
        ├── time_series_model.onnx
        ├── test_data.csv
        ├── test_labels.csv
        ├── setup_and_build.sh
        ├── run_inference.sh
        └── build/
            └── onnx_inference           # 実行ファイル
```

### 技術スタック

| レイヤー | 技術 |
|---------|------|
| **Python** | scikit-learn, pandas, numpy |
| **変換** | skl2onnx, onnx |
| **C++推論** | ONNX Runtime C++ API |
| **ビルド** | CMake, g++ |
| **環境** | WSL2 Ubuntu 22.04 |
| **IDE** | VS Code + WSL拡張機能 |

---

**作成日**: 2025-11-02  
**バージョン**: 1.0  
**対象**: C++初心者〜中級者

🎉 **お疲れさまでした！成功をお祈りします！** 🎉

